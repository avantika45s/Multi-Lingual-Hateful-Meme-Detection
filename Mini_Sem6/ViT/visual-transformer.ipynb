{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11395868,"sourceType":"datasetVersion","datasetId":7137087}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import ViTModel, ViTFeatureExtractor\nfrom transformers import BertTokenizer, BertModel\nimport re\nimport glob\n\n# Set device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load dataset\ndf = pd.read_csv('/kaggle/input/multilingual-meme-datasets/final_datasets.csv')\nprint(f\"Dataset loaded with shape: {df.shape}\")\n\n# Image directory\nimage_dir = \"/kaggle/input/multilingual-meme-datasets/datasets/datasets\"\n\n# Create a mapping between dataset 'name' and actual image filenames\ndef create_image_mapping(dataframe, image_directory):\n    \"\"\"\n    Create a mapping between dataset 'name' column and actual image filenames in the directory.\n    This handles the case where image filenames might have different formats.\n    \"\"\"\n    # Get all image files in the directory\n    image_files = glob.glob(os.path.join(image_directory, '*.*'))\n    image_mapping = {}\n    \n    # Create a set of all available image filenames (without path)\n    available_images = {os.path.basename(f) for f in image_files}\n    print(f\"Found {len(available_images)} images in directory\")\n    \n    # Print some sample images to understand the naming pattern\n    print(\"Sample image filenames:\", list(available_images)[:5])\n    \n    # Print some sample names from the dataset\n    print(\"Sample names from dataset:\", dataframe['name'].iloc[:5].tolist())\n    \n    # Method 1: Exact match\n    for name in dataframe['name'].unique():\n        if name in available_images:\n            image_mapping[name] = name\n    \n    # Method 2: Check if the id is part of the filename\n    unmapped_names = set(dataframe['name']) - set(image_mapping.keys())\n    for name in unmapped_names:\n        # Extract ID from name (assuming name has some ID pattern)\n        id_match = re.search(r'\\d+', name)\n        if id_match:\n            id_value = id_match.group()\n            # Look for files containing this ID\n            matching_files = [f for f in available_images if id_value in f]\n            if matching_files:\n                image_mapping[name] = matching_files[0]\n    \n    # Method 3: Try matching using 'ids' or 'id' column if available\n    if 'ids' in dataframe.columns or 'id' in dataframe.columns:\n        id_col = 'ids' if 'ids' in dataframe.columns else 'id'\n        id_to_name = dict(zip(dataframe[id_col], dataframe['name']))\n        \n        for id_value, name in id_to_name.items():\n            if name not in image_mapping:\n                # Look for files containing this ID\n                matching_files = [f for f in available_images if str(id_value) in f]\n                if matching_files:\n                    image_mapping[name] = matching_files[0]\n    \n    print(f\"Successfully mapped {len(image_mapping)} out of {len(dataframe['name'].unique())} unique names\")\n    return image_mapping\n\n# Custom dataset class with image mapping\nclass HarmfulContentDataset(Dataset):\n    def __init__(self, dataframe, image_dir, feature_extractor, tokenizer, image_mapping=None, max_len=128):\n        self.dataframe = dataframe\n        self.image_dir = image_dir\n        self.feature_extractor = feature_extractor\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.image_mapping = image_mapping or {}\n        \n        # Create a default blank image to use when an image is not found\n        self.blank_image = Image.new('RGB', (224, 224), color='white')\n        \n        # Image transformation\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ])\n    \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        \n        # Text features\n        text_features = f\"{row['gender']} {row['age']} {row['age_bucket']} {row['dominant_emotion']} {row['dominant_race']} {row['translated_text']}\"\n        \n        # Tokenize text\n        encoding = self.tokenizer.encode_plus(\n            text_features,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        \n        # Image processing\n        try:\n            # Get the correct image filename using the mapping\n            image_filename = self.image_mapping.get(row['name'], row['name'])\n            image_path = os.path.join(self.image_dir, image_filename)\n            \n            # Check if the file exists\n            if os.path.exists(image_path):\n                image = Image.open(image_path).convert('RGB')\n            else:\n                # Try alternate approach: check if the file exists with different extensions\n                base_name = os.path.splitext(image_path)[0]\n                for ext in ['.jpg', '.jpeg', '.png', '.gif']:\n                    alt_path = base_name + ext\n                    if os.path.exists(alt_path):\n                        image = Image.open(alt_path).convert('RGB')\n                        break\n                else:\n                    # If still not found, use blank image\n                    image = self.blank_image\n                    if idx % 100 == 0:  # Limit logging to avoid flooding\n                        print(f\"Image not found for {row['name']}, using blank image\")\n            \n            # Process image for ViT\n            image_features = self.feature_extractor(images=image, return_tensors=\"pt\")\n            pixel_values = image_features.pixel_values.squeeze()\n            \n        except Exception as e:\n            if idx % 100 == 0:  # Limit logging\n                print(f\"Error processing image for {row['name']}: {e}\")\n            # Create blank image features\n            pixel_values = torch.zeros((3, 224, 224))\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'pixel_values': pixel_values,\n            'labels': torch.tensor(row['label'], dtype=torch.long)\n        }\n\n# Custom model class (ViT + BERT + classification head)\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, num_classes=2):\n        super(MultimodalClassifier, self).__init__()\n        \n        # Load pre-trained Vision Transformer\n        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n        \n        # Load pre-trained BERT\n        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n        \n        # Freeze ViT weights\n        for param in self.vit.parameters():\n            param.requires_grad = False\n            \n        # Unfreeze the last few layers of ViT\n        for param in self.vit.encoder.layer[-2:].parameters():\n            param.requires_grad = True\n        \n        # Freeze BERT weights\n        for param in self.bert.parameters():\n            param.requires_grad = False\n            \n        # Unfreeze the last few layers of BERT\n        for param in self.bert.encoder.layer[-2:].parameters():\n            param.requires_grad = True\n        \n        # Dimensionality of ViT and BERT embeddings\n        vit_hidden_size = self.vit.config.hidden_size\n        bert_hidden_size = self.bert.config.hidden_size\n        \n        # Enhanced classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(vit_hidden_size + bert_hidden_size, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_classes)\n        )\n        \n    def forward(self, input_ids, attention_mask, pixel_values):\n        # Process image with ViT\n        vit_outputs = self.vit(pixel_values=pixel_values)\n        vit_embeddings = vit_outputs.last_hidden_state[:, 0, :]  # CLS token\n        \n        # Process text with BERT\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_embeddings = bert_outputs.last_hidden_state[:, 0, :]  # CLS token\n        \n        # Concatenate image and text features\n        combined_embeddings = torch.cat((vit_embeddings, bert_embeddings), dim=1)\n        \n        # Classification\n        logits = self.classifier(combined_embeddings)\n        \n        return logits\n\n# Training function\ndef train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10):\n    train_losses = []\n    valid_losses = []\n    train_accuracies = []\n    valid_accuracies = []\n    \n    best_valid_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        running_loss = 0.0\n        correct_train = 0\n        total_train = 0\n        \n        for batch_idx, batch in enumerate(train_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            labels = batch['labels'].to(device)\n            \n            optimizer.zero_grad()\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n            \n            # Print batch progress\n            if (batch_idx + 1) % 10 == 0:\n                print(f'Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n        \n        train_loss = running_loss / len(train_loader)\n        train_accuracy = correct_train / total_train\n        train_losses.append(train_loss)\n        train_accuracies.append(train_accuracy)\n        \n        # Validation\n        model.eval()\n        running_valid_loss = 0.0\n        correct_valid = 0\n        total_valid = 0\n        \n        with torch.no_grad():\n            for batch in valid_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                pixel_values = batch['pixel_values'].to(device)\n                labels = batch['labels'].to(device)\n                \n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n                loss = criterion(outputs, labels)\n                \n                running_valid_loss += loss.item()\n                \n                _, predicted = torch.max(outputs.data, 1)\n                total_valid += labels.size(0)\n                correct_valid += (predicted == labels).sum().item()\n        \n        valid_loss = running_valid_loss / len(valid_loader)\n        valid_accuracy = correct_valid / total_valid\n        valid_losses.append(valid_loss)\n        valid_accuracies.append(valid_accuracy)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}, '\n              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n              f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n        \n        # Save best model\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'best_multimodal_model.pth')\n            print(f'Saved model with validation loss: {valid_loss:.4f}')\n    \n    return train_losses, valid_losses, train_accuracies, valid_accuracies\n\n# Function for evaluation and metrics\ndef evaluate_model(model, test_loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n            _, predicted = torch.max(outputs.data, 1)\n            \n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(predicted.cpu().numpy())\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    class_report = classification_report(y_true, y_pred, target_names=['Non-Harmful', 'Harmful'])\n    \n    print(f'Test Accuracy: {accuracy:.4f}')\n    print('\\nConfusion Matrix:')\n    print(conf_matrix)\n    print('\\nClassification Report:')\n    print(class_report)\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n                xticklabels=['Non-Harmful', 'Harmful'], \n                yticklabels=['Non-Harmful', 'Harmful'])\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.savefig('confusion_matrix.png')\n    plt.close()\n    \n    return accuracy, conf_matrix, class_report\n\n# Function to plot training curves\ndef plot_training_curves(train_losses, valid_losses, train_accuracies, valid_accuracies):\n    # Plot loss\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(valid_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training vs Validation Loss')\n    plt.legend()\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accuracies, label='Training Accuracy')\n    plt.plot(valid_accuracies, label='Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Training vs Validation Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_curves.png')\n    plt.close()\n\n# Function to inspect dataset\ndef inspect_dataset(df, image_dir):\n    \"\"\"\n    Inspects the dataset and the image directory to understand the structure.\n    \"\"\"\n    print(\"\\n=== Dataset Inspection ===\")\n    print(f\"Dataset columns: {df.columns.tolist()}\")\n    print(f\"Number of rows: {len(df)}\")\n    print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n    \n    # Check if image directory exists\n    if not os.path.exists(image_dir):\n        print(f\"WARNING: Image directory does not exist: {image_dir}\")\n        return\n    \n    # Count image files\n    image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f)) and \n                   f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n    \n    print(f\"Number of image files in directory: {len(image_files)}\")\n    if len(image_files) > 0:\n        print(f\"Sample image filenames: {image_files[:5]}\")\n    \n    # Check name column format\n    print(\"\\nSample names from dataset:\")\n    print(df['name'].head(5).tolist())\n    \n    # Check if any dataset names exactly match image filenames\n    matching_names = [name for name in df['name'].unique() if name in image_files]\n    print(f\"Number of exact matches between 'name' column and image filenames: {len(matching_names)}\")\n    \n    # Check if IDs are in the name column\n    if 'id' in df.columns or 'ids' in df.columns:\n        id_col = 'ids' if 'ids' in df.columns else 'id'\n        print(f\"\\nSample values from '{id_col}' column:\")\n        print(df[id_col].head(5).tolist())\n        \n        # Check if IDs are in image filenames\n        sample_ids = df[id_col].astype(str).head(5).tolist()\n        for id_val in sample_ids:\n            matches = [f for f in image_files if id_val in f]\n            if matches:\n                print(f\"Found matches for ID {id_val}: {matches[:2]}\")\n                \n    return image_files\n\n# Main execution\ndef main():\n    # Initialize feature extractor and tokenizer\n    feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n    \n    # Inspect dataset and image directory\n    image_files = inspect_dataset(df, image_dir)\n    \n    # Create mapping between dataset names and image filenames\n    image_mapping = create_image_mapping(df, image_dir)\n    \n    # Split dataset\n    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n    \n    print(f\"Train set: {len(train_df)}, Validation set: {len(valid_df)}, Test set: {len(test_df)}\")\n    \n    # Create datasets with image mapping\n    train_dataset = HarmfulContentDataset(train_df, image_dir, feature_extractor, tokenizer, image_mapping)\n    valid_dataset = HarmfulContentDataset(valid_df, image_dir, feature_extractor, tokenizer, image_mapping)\n    test_dataset = HarmfulContentDataset(test_df, image_dir, feature_extractor, tokenizer, image_mapping)\n    \n    # Create dataloaders with appropriate batch size based on available GPU memory\n    batch_size = 8  # Reduced batch size to handle larger model\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)\n    \n    # Initialize model\n    model = MultimodalClassifier(num_classes=2).to(device)\n    \n    # Loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n    \n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n    \n    # Train model\n    print(\"Starting training...\")\n    train_losses, valid_losses, train_accuracies, valid_accuracies = train_model(\n        model, train_loader, valid_loader, criterion, optimizer, num_epochs=10\n    )\n    \n    # Plot training curves\n    plot_training_curves(train_losses, valid_losses, train_accuracies, valid_accuracies)\n    \n    # Load best model for evaluation\n    model.load_state_dict(torch.load('best_multimodal_model.pth'))\n    \n    # Evaluate model\n    print(\"\\nEvaluating model on test set...\")\n    accuracy, conf_matrix, class_report = evaluate_model(model, test_loader)\n    \n    # Save final model with metadata\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'accuracy': accuracy,\n        'conf_matrix': conf_matrix,\n        'class_report': class_report,\n    }, 'multimodal_harmful_content_classifier.pth')\n    \n    print(\"Model training and evaluation complete!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:56:38.930875Z","iopub.execute_input":"2025-04-16T06:56:38.931527Z","iopub.status.idle":"2025-04-16T08:33:14.005174Z","shell.execute_reply.started":"2025-04-16T06:56:38.931502Z","shell.execute_reply":"2025-04-16T08:33:14.004350Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda:0\nDataset loaded with shape: (25600, 11)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n=== Dataset Inspection ===\nDataset columns: ['ids', 'name', 'text', 'label', 'id', 'gender', 'age', 'age_bucket', 'dominant_emotion', 'dominant_race', 'translated_text']\nNumber of rows: 25600\nLabel distribution: {1: 17388, 0: 8212}\nNumber of image files in directory: 25716\nSample image filenames: ['eng476.png', 'meme_184.png', 'tangaila (166).jpg', 'Image- (178).jpg', 'Image- (2026).jpg']\n\nSample names from dataset:\n['tangaila (1).jpg', 'tangaila (2).jpg', 'tangaila (3).jpg', 'tangaila (4).jpg', 'tangaila (5).jpg']\nNumber of exact matches between 'name' column and image filenames: 18670\n\nSample values from 'ids' column:\n[1, 2, 3, 4, 5]\nFound matches for ID 1: ['meme_184.png', 'tangaila (166).jpg']\nFound matches for ID 2: ['Image- (2026).jpg', 'image_ (2682).jpg']\nFound matches for ID 3: ['Bangla Thug Life (374).jpg', '37825.png']\nFound matches for ID 4: ['eng476.png', 'meme_184.png']\nFound matches for ID 5: ['52691.png', '37825.png']\nFound 25716 images in directory\nSample image filenames: ['image_ (2440).jpg', 'image_ (1394).jpg', 'gay80.png', '06875.png', '34695.png']\nSample names from dataset: ['tangaila (1).jpg', 'tangaila (2).jpg', 'tangaila (3).jpg', 'tangaila (4).jpg', 'tangaila (5).jpg']\nSuccessfully mapped 25432 out of 25432 unique names\nTrain set: 17920, Validation set: 3840, Test set: 3840\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Batch 10/2240, Loss: 0.6757\nEpoch 1, Batch 20/2240, Loss: 0.5741\nEpoch 1, Batch 30/2240, Loss: 0.6345\nEpoch 1, Batch 40/2240, Loss: 0.5359\nEpoch 1, Batch 50/2240, Loss: 0.5310\nEpoch 1, Batch 60/2240, Loss: 0.6471\nEpoch 1, Batch 70/2240, Loss: 0.7478\nEpoch 1, Batch 80/2240, Loss: 0.4174\nEpoch 1, Batch 90/2240, Loss: 0.4602\nEpoch 1, Batch 100/2240, Loss: 0.5193\nEpoch 1, Batch 110/2240, Loss: 0.5933\nEpoch 1, Batch 120/2240, Loss: 0.5653\nEpoch 1, Batch 130/2240, Loss: 0.3392\nEpoch 1, Batch 140/2240, Loss: 0.6926\nEpoch 1, Batch 150/2240, Loss: 0.7081\nEpoch 1, Batch 160/2240, Loss: 0.3805\nEpoch 1, Batch 170/2240, Loss: 0.5094\nEpoch 1, Batch 180/2240, Loss: 0.4571\nEpoch 1, Batch 190/2240, Loss: 0.5129\nEpoch 1, Batch 200/2240, Loss: 0.6089\nEpoch 1, Batch 210/2240, Loss: 0.5802\nEpoch 1, Batch 220/2240, Loss: 0.6602\nEpoch 1, Batch 230/2240, Loss: 0.5168\nEpoch 1, Batch 240/2240, Loss: 0.6039\nEpoch 1, Batch 250/2240, Loss: 0.4914\nEpoch 1, Batch 260/2240, Loss: 0.4597\nEpoch 1, Batch 270/2240, Loss: 0.8202\nEpoch 1, Batch 280/2240, Loss: 0.3366\nEpoch 1, Batch 290/2240, Loss: 0.4520\nEpoch 1, Batch 300/2240, Loss: 0.4153\nEpoch 1, Batch 310/2240, Loss: 0.6855\nEpoch 1, Batch 320/2240, Loss: 0.5888\nEpoch 1, Batch 330/2240, Loss: 0.5947\nEpoch 1, Batch 340/2240, Loss: 0.5036\nEpoch 1, Batch 350/2240, Loss: 0.2552\nEpoch 1, Batch 360/2240, Loss: 0.6682\nEpoch 1, Batch 370/2240, Loss: 0.3891\nEpoch 1, Batch 380/2240, Loss: 0.5201\nEpoch 1, Batch 390/2240, Loss: 0.5397\nEpoch 1, Batch 400/2240, Loss: 0.6800\nEpoch 1, Batch 410/2240, Loss: 0.5654\nEpoch 1, Batch 420/2240, Loss: 0.3212\nEpoch 1, Batch 430/2240, Loss: 0.4346\nEpoch 1, Batch 440/2240, Loss: 0.5032\nEpoch 1, Batch 450/2240, Loss: 0.4676\nEpoch 1, Batch 460/2240, Loss: 0.3226\nEpoch 1, Batch 470/2240, Loss: 0.1093\nEpoch 1, Batch 480/2240, Loss: 0.4858\nEpoch 1, Batch 490/2240, Loss: 0.3400\nEpoch 1, Batch 500/2240, Loss: 0.3832\nEpoch 1, Batch 510/2240, Loss: 0.1879\nEpoch 1, Batch 520/2240, Loss: 0.6520\nEpoch 1, Batch 530/2240, Loss: 0.5487\nEpoch 1, Batch 540/2240, Loss: 0.3586\nEpoch 1, Batch 550/2240, Loss: 0.6951\nEpoch 1, Batch 560/2240, Loss: 0.7383\nEpoch 1, Batch 570/2240, Loss: 0.5567\nEpoch 1, Batch 580/2240, Loss: 0.5383\nEpoch 1, Batch 590/2240, Loss: 0.5803\nEpoch 1, Batch 600/2240, Loss: 0.6174\nEpoch 1, Batch 610/2240, Loss: 0.4059\nEpoch 1, Batch 620/2240, Loss: 0.4502\nEpoch 1, Batch 630/2240, Loss: 0.2173\nEpoch 1, Batch 640/2240, Loss: 0.5290\nEpoch 1, Batch 650/2240, Loss: 0.6376\nEpoch 1, Batch 660/2240, Loss: 0.3310\nEpoch 1, Batch 670/2240, Loss: 0.5529\nEpoch 1, Batch 680/2240, Loss: 0.5650\nEpoch 1, Batch 690/2240, Loss: 0.5108\nEpoch 1, Batch 700/2240, Loss: 0.1915\nEpoch 1, Batch 710/2240, Loss: 0.3996\nEpoch 1, Batch 720/2240, Loss: 0.4037\nEpoch 1, Batch 730/2240, Loss: 0.3045\nEpoch 1, Batch 740/2240, Loss: 0.5629\nEpoch 1, Batch 750/2240, Loss: 0.7734\nEpoch 1, Batch 760/2240, Loss: 0.1950\nEpoch 1, Batch 770/2240, Loss: 0.4848\nEpoch 1, Batch 780/2240, Loss: 0.3471\nEpoch 1, Batch 790/2240, Loss: 0.3290\nEpoch 1, Batch 800/2240, Loss: 0.5253\nEpoch 1, Batch 810/2240, Loss: 0.6215\nEpoch 1, Batch 820/2240, Loss: 0.4897\nEpoch 1, Batch 830/2240, Loss: 0.4932\nEpoch 1, Batch 840/2240, Loss: 0.6788\nEpoch 1, Batch 850/2240, Loss: 0.8435\nEpoch 1, Batch 860/2240, Loss: 0.3925\nEpoch 1, Batch 870/2240, Loss: 0.3818\nEpoch 1, Batch 880/2240, Loss: 0.4133\nEpoch 1, Batch 890/2240, Loss: 0.3345\nEpoch 1, Batch 900/2240, Loss: 0.4142\nEpoch 1, Batch 910/2240, Loss: 0.7201\nEpoch 1, Batch 920/2240, Loss: 0.8854\nEpoch 1, Batch 930/2240, Loss: 0.4361\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n  warnings.warn(str(msg))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Batch 940/2240, Loss: 0.3253\nEpoch 1, Batch 950/2240, Loss: 0.3089\nEpoch 1, Batch 960/2240, Loss: 0.5901\nEpoch 1, Batch 970/2240, Loss: 0.6687\nEpoch 1, Batch 980/2240, Loss: 0.7747\nEpoch 1, Batch 990/2240, Loss: 0.4309\nEpoch 1, Batch 1000/2240, Loss: 0.4488\nEpoch 1, Batch 1010/2240, Loss: 0.6312\nEpoch 1, Batch 1020/2240, Loss: 0.4252\nEpoch 1, Batch 1030/2240, Loss: 0.4047\nEpoch 1, Batch 1040/2240, Loss: 0.6677\nEpoch 1, Batch 1050/2240, Loss: 0.3503\nEpoch 1, Batch 1060/2240, Loss: 0.3032\nEpoch 1, Batch 1070/2240, Loss: 0.5312\nEpoch 1, Batch 1080/2240, Loss: 0.3762\nEpoch 1, Batch 1090/2240, Loss: 0.4282\nEpoch 1, Batch 1100/2240, Loss: 0.4268\nEpoch 1, Batch 1110/2240, Loss: 0.3803\nEpoch 1, Batch 1120/2240, Loss: 0.5010\nEpoch 1, Batch 1130/2240, Loss: 1.0056\nEpoch 1, Batch 1140/2240, Loss: 0.3192\nEpoch 1, Batch 1150/2240, Loss: 0.8067\nEpoch 1, Batch 1160/2240, Loss: 0.3430\nEpoch 1, Batch 1170/2240, Loss: 0.3663\nEpoch 1, Batch 1180/2240, Loss: 0.5317\nEpoch 1, Batch 1190/2240, Loss: 0.6463\nEpoch 1, Batch 1200/2240, Loss: 0.5210\nEpoch 1, Batch 1210/2240, Loss: 0.7988\nEpoch 1, Batch 1220/2240, Loss: 0.2755\nEpoch 1, Batch 1230/2240, Loss: 0.7283\nEpoch 1, Batch 1240/2240, Loss: 0.3077\nEpoch 1, Batch 1250/2240, Loss: 0.4833\nEpoch 1, Batch 1260/2240, Loss: 0.5594\nEpoch 1, Batch 1270/2240, Loss: 0.2356\nEpoch 1, Batch 1280/2240, Loss: 0.3755\nEpoch 1, Batch 1290/2240, Loss: 0.4958\nEpoch 1, Batch 1300/2240, Loss: 0.3385\nEpoch 1, Batch 1310/2240, Loss: 0.3842\nEpoch 1, Batch 1320/2240, Loss: 0.3136\nEpoch 1, Batch 1330/2240, Loss: 0.0830\nEpoch 1, Batch 1340/2240, Loss: 0.2456\nEpoch 1, Batch 1350/2240, Loss: 0.3489\nEpoch 1, Batch 1360/2240, Loss: 0.2199\nEpoch 1, Batch 1370/2240, Loss: 0.3126\nEpoch 1, Batch 1380/2240, Loss: 0.3523\nEpoch 1, Batch 1390/2240, Loss: 0.4154\nEpoch 1, Batch 1400/2240, Loss: 0.4201\nEpoch 1, Batch 1410/2240, Loss: 0.4587\nEpoch 1, Batch 1420/2240, Loss: 0.3962\nEpoch 1, Batch 1430/2240, Loss: 0.4118\nEpoch 1, Batch 1440/2240, Loss: 0.3336\nEpoch 1, Batch 1450/2240, Loss: 0.3848\nEpoch 1, Batch 1460/2240, Loss: 0.6764\nEpoch 1, Batch 1470/2240, Loss: 0.4568\nEpoch 1, Batch 1480/2240, Loss: 0.6509\nEpoch 1, Batch 1490/2240, Loss: 0.5471\nEpoch 1, Batch 1500/2240, Loss: 0.2773\nEpoch 1, Batch 1510/2240, Loss: 0.4022\nEpoch 1, Batch 1520/2240, Loss: 0.2727\nEpoch 1, Batch 1530/2240, Loss: 0.5490\nEpoch 1, Batch 1540/2240, Loss: 0.4112\nEpoch 1, Batch 1550/2240, Loss: 0.3268\nEpoch 1, Batch 1560/2240, Loss: 0.5226\nEpoch 1, Batch 1570/2240, Loss: 0.6875\nEpoch 1, Batch 1580/2240, Loss: 0.4634\nEpoch 1, Batch 1590/2240, Loss: 0.5061\nEpoch 1, Batch 1600/2240, Loss: 0.3930\nEpoch 1, Batch 1610/2240, Loss: 0.2044\nEpoch 1, Batch 1620/2240, Loss: 0.3195\nEpoch 1, Batch 1630/2240, Loss: 0.1636\nEpoch 1, Batch 1640/2240, Loss: 0.2396\nEpoch 1, Batch 1650/2240, Loss: 0.3847\nEpoch 1, Batch 1660/2240, Loss: 0.5137\nEpoch 1, Batch 1670/2240, Loss: 0.6971\nEpoch 1, Batch 1680/2240, Loss: 0.3203\nEpoch 1, Batch 1690/2240, Loss: 0.3718\nEpoch 1, Batch 1700/2240, Loss: 0.3624\nEpoch 1, Batch 1710/2240, Loss: 0.4608\nEpoch 1, Batch 1720/2240, Loss: 0.5934\nEpoch 1, Batch 1730/2240, Loss: 0.1512\nEpoch 1, Batch 1740/2240, Loss: 0.8356\nEpoch 1, Batch 1750/2240, Loss: 0.3174\nEpoch 1, Batch 1760/2240, Loss: 0.3038\nEpoch 1, Batch 1770/2240, Loss: 0.5540\nEpoch 1, Batch 1780/2240, Loss: 0.4003\nEpoch 1, Batch 1790/2240, Loss: 0.4128\nEpoch 1, Batch 1800/2240, Loss: 0.3000\nEpoch 1, Batch 1810/2240, Loss: 0.4557\nEpoch 1, Batch 1820/2240, Loss: 0.3361\nEpoch 1, Batch 1830/2240, Loss: 0.5334\nEpoch 1, Batch 1840/2240, Loss: 0.2868\nEpoch 1, Batch 1850/2240, Loss: 0.3989\nEpoch 1, Batch 1860/2240, Loss: 0.2192\nEpoch 1, Batch 1870/2240, Loss: 0.4082\nEpoch 1, Batch 1880/2240, Loss: 0.6925\nEpoch 1, Batch 1890/2240, Loss: 0.3486\nEpoch 1, Batch 1900/2240, Loss: 0.5231\nEpoch 1, Batch 1910/2240, Loss: 0.7030\nEpoch 1, Batch 1920/2240, Loss: 0.1231\nEpoch 1, Batch 1930/2240, Loss: 0.2111\nEpoch 1, Batch 1940/2240, Loss: 0.5936\nEpoch 1, Batch 1950/2240, Loss: 0.3814\nEpoch 1, Batch 1960/2240, Loss: 0.6139\nEpoch 1, Batch 1970/2240, Loss: 0.2253\nEpoch 1, Batch 1980/2240, Loss: 0.5136\nEpoch 1, Batch 1990/2240, Loss: 0.7365\nEpoch 1, Batch 2000/2240, Loss: 0.4073\nEpoch 1, Batch 2010/2240, Loss: 0.2501\nEpoch 1, Batch 2020/2240, Loss: 0.5746\nEpoch 1, Batch 2030/2240, Loss: 0.6061\nEpoch 1, Batch 2040/2240, Loss: 0.4126\nEpoch 1, Batch 2050/2240, Loss: 0.4674\nEpoch 1, Batch 2060/2240, Loss: 0.2800\nEpoch 1, Batch 2070/2240, Loss: 0.4903\nEpoch 1, Batch 2080/2240, Loss: 0.1403\nEpoch 1, Batch 2090/2240, Loss: 0.4832\nEpoch 1, Batch 2100/2240, Loss: 0.3169\nEpoch 1, Batch 2110/2240, Loss: 0.3349\nEpoch 1, Batch 2120/2240, Loss: 0.5308\nEpoch 1, Batch 2130/2240, Loss: 0.4886\nEpoch 1, Batch 2140/2240, Loss: 0.6286\nEpoch 1, Batch 2150/2240, Loss: 0.4121\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Batch 2160/2240, Loss: 0.4712\nEpoch 1, Batch 2170/2240, Loss: 0.5092\nEpoch 1, Batch 2180/2240, Loss: 0.1724\nEpoch 1, Batch 2190/2240, Loss: 0.7835\nEpoch 1, Batch 2200/2240, Loss: 0.4754\nEpoch 1, Batch 2210/2240, Loss: 0.3262\nEpoch 1, Batch 2220/2240, Loss: 0.3588\nEpoch 1, Batch 2230/2240, Loss: 0.7274\nEpoch 1, Batch 2240/2240, Loss: 0.3962\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Train Loss: 0.4610, Train Acc: 0.7584, Valid Loss: 0.4097, Valid Acc: 0.7969\nSaved model with validation loss: 0.4097\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Batch 10/2240, Loss: 0.3980\nEpoch 2, Batch 20/2240, Loss: 0.5079\nEpoch 2, Batch 30/2240, Loss: 0.4378\nEpoch 2, Batch 40/2240, Loss: 0.3560\nEpoch 2, Batch 50/2240, Loss: 0.2851\nEpoch 2, Batch 60/2240, Loss: 0.4006\nEpoch 2, Batch 70/2240, Loss: 0.3237\nEpoch 2, Batch 80/2240, Loss: 0.1248\nEpoch 2, Batch 90/2240, Loss: 0.1026\nEpoch 2, Batch 100/2240, Loss: 0.2092\nEpoch 2, Batch 110/2240, Loss: 0.2588\nEpoch 2, Batch 120/2240, Loss: 0.6170\nEpoch 2, Batch 130/2240, Loss: 0.1668\nEpoch 2, Batch 140/2240, Loss: 0.1932\nEpoch 2, Batch 150/2240, Loss: 0.2652\nEpoch 2, Batch 160/2240, Loss: 0.6042\nEpoch 2, Batch 170/2240, Loss: 0.3773\nEpoch 2, Batch 180/2240, Loss: 0.3564\nEpoch 2, Batch 190/2240, Loss: 0.5586\nEpoch 2, Batch 200/2240, Loss: 0.2675\nEpoch 2, Batch 210/2240, Loss: 0.2829\nEpoch 2, Batch 220/2240, Loss: 0.4062\nEpoch 2, Batch 230/2240, Loss: 0.2422\nEpoch 2, Batch 240/2240, Loss: 0.3784\nEpoch 2, Batch 250/2240, Loss: 0.4484\nEpoch 2, Batch 260/2240, Loss: 0.1809\nEpoch 2, Batch 270/2240, Loss: 0.5415\nEpoch 2, Batch 280/2240, Loss: 0.3053\nEpoch 2, Batch 290/2240, Loss: 0.2224\nEpoch 2, Batch 300/2240, Loss: 0.8925\nEpoch 2, Batch 310/2240, Loss: 0.3178\nEpoch 2, Batch 320/2240, Loss: 0.4611\nEpoch 2, Batch 330/2240, Loss: 0.3459\nEpoch 2, Batch 340/2240, Loss: 0.4955\nEpoch 2, Batch 350/2240, Loss: 0.5225\nEpoch 2, Batch 360/2240, Loss: 0.1216\nEpoch 2, Batch 370/2240, Loss: 0.3572\nEpoch 2, Batch 380/2240, Loss: 0.3218\nEpoch 2, Batch 390/2240, Loss: 0.3801\nEpoch 2, Batch 400/2240, Loss: 0.0878\nEpoch 2, Batch 410/2240, Loss: 0.3167\nEpoch 2, Batch 420/2240, Loss: 0.3859\nEpoch 2, Batch 430/2240, Loss: 0.4319\nEpoch 2, Batch 440/2240, Loss: 0.2453\nEpoch 2, Batch 450/2240, Loss: 0.6697\nEpoch 2, Batch 460/2240, Loss: 0.0929\nEpoch 2, Batch 470/2240, Loss: 0.2783\nEpoch 2, Batch 480/2240, Loss: 0.3036\nEpoch 2, Batch 490/2240, Loss: 0.4752\nEpoch 2, Batch 500/2240, Loss: 0.2354\nEpoch 2, Batch 510/2240, Loss: 0.1969\nEpoch 2, Batch 520/2240, Loss: 0.2824\nEpoch 2, Batch 530/2240, Loss: 0.6348\nEpoch 2, Batch 540/2240, Loss: 0.3240\nEpoch 2, Batch 550/2240, Loss: 0.2272\nEpoch 2, Batch 560/2240, Loss: 0.5550\nEpoch 2, Batch 570/2240, Loss: 0.1177\nEpoch 2, Batch 580/2240, Loss: 0.3319\nEpoch 2, Batch 590/2240, Loss: 0.3801\nEpoch 2, Batch 600/2240, Loss: 0.8584\nEpoch 2, Batch 610/2240, Loss: 0.3990\nEpoch 2, Batch 620/2240, Loss: 0.3926\nEpoch 2, Batch 630/2240, Loss: 0.3446\nEpoch 2, Batch 640/2240, Loss: 0.2057\nEpoch 2, Batch 650/2240, Loss: 0.8624\nEpoch 2, Batch 660/2240, Loss: 0.9229\nEpoch 2, Batch 670/2240, Loss: 0.4026\nEpoch 2, Batch 680/2240, Loss: 0.1907\nEpoch 2, Batch 690/2240, Loss: 0.3071\nEpoch 2, Batch 700/2240, Loss: 0.1530\nEpoch 2, Batch 710/2240, Loss: 0.4048\nEpoch 2, Batch 720/2240, Loss: 0.4237\nEpoch 2, Batch 730/2240, Loss: 0.4189\nEpoch 2, Batch 740/2240, Loss: 0.4426\nEpoch 2, Batch 750/2240, Loss: 0.2454\nEpoch 2, Batch 760/2240, Loss: 0.4103\nEpoch 2, Batch 770/2240, Loss: 0.4365\nEpoch 2, Batch 780/2240, Loss: 0.3870\nEpoch 2, Batch 790/2240, Loss: 0.2606\nEpoch 2, Batch 800/2240, Loss: 0.2993\nEpoch 2, Batch 810/2240, Loss: 0.6667\nEpoch 2, Batch 820/2240, Loss: 0.5494\nEpoch 2, Batch 830/2240, Loss: 0.5016\nEpoch 2, Batch 840/2240, Loss: 0.4204\nEpoch 2, Batch 850/2240, Loss: 0.7610\nEpoch 2, Batch 860/2240, Loss: 0.3700\nEpoch 2, Batch 870/2240, Loss: 0.5166\nEpoch 2, Batch 880/2240, Loss: 0.2178\nEpoch 2, Batch 890/2240, Loss: 0.4050\nEpoch 2, Batch 900/2240, Loss: 0.7734\nEpoch 2, Batch 910/2240, Loss: 0.1293\nEpoch 2, Batch 920/2240, Loss: 0.3995\nEpoch 2, Batch 930/2240, Loss: 0.3006\nEpoch 2, Batch 940/2240, Loss: 0.4789\nEpoch 2, Batch 950/2240, Loss: 0.3416\nEpoch 2, Batch 960/2240, Loss: 0.3093\nEpoch 2, Batch 970/2240, Loss: 0.4908\nEpoch 2, Batch 980/2240, Loss: 0.1888\nEpoch 2, Batch 990/2240, Loss: 0.4135\nEpoch 2, Batch 1000/2240, Loss: 0.1562\nEpoch 2, Batch 1010/2240, Loss: 0.2949\nEpoch 2, Batch 1020/2240, Loss: 0.5925\nEpoch 2, Batch 1030/2240, Loss: 0.5411\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n  warnings.warn(str(msg))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Batch 1040/2240, Loss: 0.4165\nEpoch 2, Batch 1050/2240, Loss: 0.2411\nEpoch 2, Batch 1060/2240, Loss: 0.2515\nEpoch 2, Batch 1070/2240, Loss: 0.3710\nEpoch 2, Batch 1080/2240, Loss: 0.0758\nEpoch 2, Batch 1090/2240, Loss: 0.6703\nEpoch 2, Batch 1100/2240, Loss: 0.8295\nEpoch 2, Batch 1110/2240, Loss: 0.1020\nEpoch 2, Batch 1120/2240, Loss: 0.3714\nEpoch 2, Batch 1130/2240, Loss: 0.3089\nEpoch 2, Batch 1140/2240, Loss: 0.4208\nEpoch 2, Batch 1150/2240, Loss: 0.4137\nEpoch 2, Batch 1160/2240, Loss: 0.1873\nEpoch 2, Batch 1170/2240, Loss: 0.0870\nEpoch 2, Batch 1180/2240, Loss: 0.7496\nEpoch 2, Batch 1190/2240, Loss: 0.4533\nEpoch 2, Batch 1200/2240, Loss: 0.2540\nEpoch 2, Batch 1210/2240, Loss: 0.3407\nEpoch 2, Batch 1220/2240, Loss: 0.4232\nEpoch 2, Batch 1230/2240, Loss: 0.2343\nEpoch 2, Batch 1240/2240, Loss: 0.1623\nEpoch 2, Batch 1250/2240, Loss: 0.6485\nEpoch 2, Batch 1260/2240, Loss: 0.3532\nEpoch 2, Batch 1270/2240, Loss: 0.4192\nEpoch 2, Batch 1280/2240, Loss: 0.4605\nEpoch 2, Batch 1290/2240, Loss: 0.3792\nEpoch 2, Batch 1300/2240, Loss: 0.3553\nEpoch 2, Batch 1310/2240, Loss: 0.2398\nEpoch 2, Batch 1320/2240, Loss: 0.4097\nEpoch 2, Batch 1330/2240, Loss: 0.3259\nEpoch 2, Batch 1340/2240, Loss: 0.1673\nEpoch 2, Batch 1350/2240, Loss: 0.4668\nEpoch 2, Batch 1360/2240, Loss: 0.1673\nEpoch 2, Batch 1370/2240, Loss: 0.6478\nEpoch 2, Batch 1380/2240, Loss: 0.3233\nEpoch 2, Batch 1390/2240, Loss: 0.2034\nEpoch 2, Batch 1400/2240, Loss: 0.4018\nEpoch 2, Batch 1410/2240, Loss: 0.7446\nEpoch 2, Batch 1420/2240, Loss: 0.0983\nEpoch 2, Batch 1430/2240, Loss: 0.2930\nEpoch 2, Batch 1440/2240, Loss: 0.6425\nEpoch 2, Batch 1450/2240, Loss: 0.2572\nEpoch 2, Batch 1460/2240, Loss: 0.3146\nEpoch 2, Batch 1470/2240, Loss: 0.2835\nEpoch 2, Batch 1480/2240, Loss: 0.2158\nEpoch 2, Batch 1490/2240, Loss: 0.6069\nEpoch 2, Batch 1500/2240, Loss: 0.2222\nEpoch 2, Batch 1510/2240, Loss: 0.6288\nEpoch 2, Batch 1520/2240, Loss: 0.1530\nEpoch 2, Batch 1530/2240, Loss: 0.3274\nEpoch 2, Batch 1540/2240, Loss: 0.2137\nEpoch 2, Batch 1550/2240, Loss: 0.4157\nEpoch 2, Batch 1560/2240, Loss: 0.3649\nEpoch 2, Batch 1570/2240, Loss: 0.1751\nEpoch 2, Batch 1580/2240, Loss: 0.2421\nEpoch 2, Batch 1590/2240, Loss: 0.3133\nEpoch 2, Batch 1600/2240, Loss: 0.3756\nEpoch 2, Batch 1610/2240, Loss: 0.3882\nEpoch 2, Batch 1620/2240, Loss: 0.1905\nEpoch 2, Batch 1630/2240, Loss: 0.4961\nEpoch 2, Batch 1640/2240, Loss: 0.4969\nEpoch 2, Batch 1650/2240, Loss: 0.5703\nEpoch 2, Batch 1660/2240, Loss: 0.4644\nEpoch 2, Batch 1670/2240, Loss: 0.4575\nEpoch 2, Batch 1680/2240, Loss: 0.7641\nEpoch 2, Batch 1690/2240, Loss: 0.4744\nEpoch 2, Batch 1700/2240, Loss: 0.5116\nEpoch 2, Batch 1710/2240, Loss: 0.2266\nEpoch 2, Batch 1720/2240, Loss: 0.8320\nEpoch 2, Batch 1730/2240, Loss: 0.3210\nEpoch 2, Batch 1740/2240, Loss: 0.4439\nEpoch 2, Batch 1750/2240, Loss: 0.7773\nEpoch 2, Batch 1760/2240, Loss: 0.2132\nEpoch 2, Batch 1770/2240, Loss: 0.2825\nEpoch 2, Batch 1780/2240, Loss: 0.5305\nEpoch 2, Batch 1790/2240, Loss: 0.3395\nEpoch 2, Batch 1800/2240, Loss: 0.6700\nEpoch 2, Batch 1810/2240, Loss: 0.2568\nEpoch 2, Batch 1820/2240, Loss: 0.3799\nEpoch 2, Batch 1830/2240, Loss: 0.5589\nEpoch 2, Batch 1840/2240, Loss: 0.5398\nEpoch 2, Batch 1850/2240, Loss: 0.2579\nEpoch 2, Batch 1860/2240, Loss: 0.3996\nEpoch 2, Batch 1870/2240, Loss: 0.3456\nEpoch 2, Batch 1880/2240, Loss: 0.3073\nEpoch 2, Batch 1890/2240, Loss: 0.2772\nEpoch 2, Batch 1900/2240, Loss: 0.2005\nEpoch 2, Batch 1910/2240, Loss: 0.3966\nEpoch 2, Batch 1920/2240, Loss: 1.1820\nEpoch 2, Batch 1930/2240, Loss: 0.2636\nEpoch 2, Batch 1940/2240, Loss: 0.4458\nEpoch 2, Batch 1950/2240, Loss: 0.3719\nEpoch 2, Batch 1960/2240, Loss: 0.2530\nEpoch 2, Batch 1970/2240, Loss: 0.3729\nEpoch 2, Batch 1980/2240, Loss: 0.2249\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Batch 1990/2240, Loss: 0.3985\nEpoch 2, Batch 2000/2240, Loss: 0.4704\nEpoch 2, Batch 2010/2240, Loss: 0.4063\nEpoch 2, Batch 2020/2240, Loss: 0.1730\nEpoch 2, Batch 2030/2240, Loss: 0.2775\nEpoch 2, Batch 2040/2240, Loss: 0.0733\nEpoch 2, Batch 2050/2240, Loss: 0.2578\nEpoch 2, Batch 2060/2240, Loss: 0.1455\nEpoch 2, Batch 2070/2240, Loss: 0.4953\nEpoch 2, Batch 2080/2240, Loss: 0.5461\nEpoch 2, Batch 2090/2240, Loss: 0.2637\nEpoch 2, Batch 2100/2240, Loss: 0.2598\nEpoch 2, Batch 2110/2240, Loss: 0.4031\nEpoch 2, Batch 2120/2240, Loss: 0.4043\nEpoch 2, Batch 2130/2240, Loss: 0.0198\nEpoch 2, Batch 2140/2240, Loss: 0.2489\nEpoch 2, Batch 2150/2240, Loss: 0.0840\nEpoch 2, Batch 2160/2240, Loss: 0.3625\nEpoch 2, Batch 2170/2240, Loss: 0.2889\nEpoch 2, Batch 2180/2240, Loss: 0.3972\nEpoch 2, Batch 2190/2240, Loss: 0.2921\nEpoch 2, Batch 2200/2240, Loss: 0.4073\nEpoch 2, Batch 2210/2240, Loss: 0.2883\nEpoch 2, Batch 2220/2240, Loss: 0.4712\nEpoch 2, Batch 2230/2240, Loss: 0.1632\nEpoch 2, Batch 2240/2240, Loss: 0.2209\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Train Loss: 0.3857, Train Acc: 0.8141, Valid Loss: 0.4126, Valid Acc: 0.8042\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Batch 10/2240, Loss: 0.2204\nEpoch 3, Batch 20/2240, Loss: 0.1694\nEpoch 3, Batch 30/2240, Loss: 0.3819\nEpoch 3, Batch 40/2240, Loss: 0.4826\nEpoch 3, Batch 50/2240, Loss: 0.2031\nEpoch 3, Batch 60/2240, Loss: 0.5790\nEpoch 3, Batch 70/2240, Loss: 0.1353\nEpoch 3, Batch 80/2240, Loss: 0.2745\nEpoch 3, Batch 90/2240, Loss: 0.4042\nEpoch 3, Batch 100/2240, Loss: 0.2451\nEpoch 3, Batch 110/2240, Loss: 0.5527\nEpoch 3, Batch 120/2240, Loss: 0.5649\nEpoch 3, Batch 130/2240, Loss: 0.5798\nEpoch 3, Batch 140/2240, Loss: 0.3348\nEpoch 3, Batch 150/2240, Loss: 0.4051\nEpoch 3, Batch 160/2240, Loss: 0.3781\nEpoch 3, Batch 170/2240, Loss: 0.5915\nEpoch 3, Batch 180/2240, Loss: 0.2566\nEpoch 3, Batch 190/2240, Loss: 0.2187\nEpoch 3, Batch 200/2240, Loss: 0.2900\nEpoch 3, Batch 210/2240, Loss: 0.4754\nEpoch 3, Batch 220/2240, Loss: 0.2149\nEpoch 3, Batch 230/2240, Loss: 0.2012\nEpoch 3, Batch 240/2240, Loss: 0.5474\nEpoch 3, Batch 250/2240, Loss: 0.3327\nEpoch 3, Batch 260/2240, Loss: 0.3914\nEpoch 3, Batch 270/2240, Loss: 0.2088\nEpoch 3, Batch 280/2240, Loss: 0.1115\nEpoch 3, Batch 290/2240, Loss: 0.3957\nEpoch 3, Batch 300/2240, Loss: 0.4292\nEpoch 3, Batch 310/2240, Loss: 0.2328\nEpoch 3, Batch 320/2240, Loss: 0.2405\nEpoch 3, Batch 330/2240, Loss: 0.3541\nEpoch 3, Batch 340/2240, Loss: 0.5424\nEpoch 3, Batch 350/2240, Loss: 0.3540\nEpoch 3, Batch 360/2240, Loss: 0.4190\nEpoch 3, Batch 370/2240, Loss: 0.1231\nEpoch 3, Batch 380/2240, Loss: 0.4833\nEpoch 3, Batch 390/2240, Loss: 0.2259\nEpoch 3, Batch 400/2240, Loss: 0.0872\nEpoch 3, Batch 410/2240, Loss: 0.3692\nEpoch 3, Batch 420/2240, Loss: 0.0941\nEpoch 3, Batch 430/2240, Loss: 0.0989\nEpoch 3, Batch 440/2240, Loss: 0.1956\nEpoch 3, Batch 450/2240, Loss: 0.3523\nEpoch 3, Batch 460/2240, Loss: 0.6247\nEpoch 3, Batch 470/2240, Loss: 0.3523\nEpoch 3, Batch 480/2240, Loss: 0.6621\nEpoch 3, Batch 490/2240, Loss: 0.2736\nEpoch 3, Batch 500/2240, Loss: 0.0879\nEpoch 3, Batch 510/2240, Loss: 0.1189\nEpoch 3, Batch 520/2240, Loss: 0.2787\nEpoch 3, Batch 530/2240, Loss: 0.2376\nEpoch 3, Batch 540/2240, Loss: 0.3241\nEpoch 3, Batch 550/2240, Loss: 0.1743\nEpoch 3, Batch 560/2240, Loss: 0.5353\nEpoch 3, Batch 570/2240, Loss: 0.3209\nEpoch 3, Batch 580/2240, Loss: 0.9713\nEpoch 3, Batch 590/2240, Loss: 0.1374\nEpoch 3, Batch 600/2240, Loss: 0.1400\nEpoch 3, Batch 610/2240, Loss: 0.5880\nEpoch 3, Batch 620/2240, Loss: 0.1668\nEpoch 3, Batch 630/2240, Loss: 0.0335\nEpoch 3, Batch 640/2240, Loss: 0.3734\nEpoch 3, Batch 650/2240, Loss: 0.4778\nEpoch 3, Batch 660/2240, Loss: 0.0780\nEpoch 3, Batch 670/2240, Loss: 0.5687\nEpoch 3, Batch 680/2240, Loss: 0.0538\nEpoch 3, Batch 690/2240, Loss: 0.1012\nEpoch 3, Batch 700/2240, Loss: 0.3011\nEpoch 3, Batch 710/2240, Loss: 0.5337\nEpoch 3, Batch 720/2240, Loss: 0.1940\nEpoch 3, Batch 730/2240, Loss: 0.7385\nEpoch 3, Batch 740/2240, Loss: 0.1332\nEpoch 3, Batch 750/2240, Loss: 0.5408\nEpoch 3, Batch 760/2240, Loss: 0.2397\nEpoch 3, Batch 770/2240, Loss: 0.8100\nEpoch 3, Batch 780/2240, Loss: 0.3741\nEpoch 3, Batch 790/2240, Loss: 0.1430\nEpoch 3, Batch 800/2240, Loss: 0.2447\nEpoch 3, Batch 810/2240, Loss: 0.1279\nEpoch 3, Batch 820/2240, Loss: 0.7605\nEpoch 3, Batch 830/2240, Loss: 0.2789\nEpoch 3, Batch 840/2240, Loss: 0.6201\nEpoch 3, Batch 850/2240, Loss: 0.2914\nEpoch 3, Batch 860/2240, Loss: 0.2274\nEpoch 3, Batch 870/2240, Loss: 0.0776\nEpoch 3, Batch 880/2240, Loss: 0.2377\nEpoch 3, Batch 890/2240, Loss: 0.3871\nEpoch 3, Batch 900/2240, Loss: 0.2759\nEpoch 3, Batch 910/2240, Loss: 0.6464\nEpoch 3, Batch 920/2240, Loss: 0.1053\nEpoch 3, Batch 930/2240, Loss: 0.7274\nEpoch 3, Batch 940/2240, Loss: 0.2754\nEpoch 3, Batch 950/2240, Loss: 0.5281\nEpoch 3, Batch 960/2240, Loss: 0.4409\nEpoch 3, Batch 970/2240, Loss: 0.5052\nEpoch 3, Batch 980/2240, Loss: 0.2659\nEpoch 3, Batch 990/2240, Loss: 0.3275\nEpoch 3, Batch 1000/2240, Loss: 0.3386\nEpoch 3, Batch 1010/2240, Loss: 0.5983\nEpoch 3, Batch 1020/2240, Loss: 0.1306\nEpoch 3, Batch 1030/2240, Loss: 0.5215\nEpoch 3, Batch 1040/2240, Loss: 0.1209\nEpoch 3, Batch 1050/2240, Loss: 0.3541\nEpoch 3, Batch 1060/2240, Loss: 0.5145\nEpoch 3, Batch 1070/2240, Loss: 1.0704\nEpoch 3, Batch 1080/2240, Loss: 0.3098\nEpoch 3, Batch 1090/2240, Loss: 0.6978\nEpoch 3, Batch 1100/2240, Loss: 0.3782\nEpoch 3, Batch 1110/2240, Loss: 0.3995\nEpoch 3, Batch 1120/2240, Loss: 0.8939\nEpoch 3, Batch 1130/2240, Loss: 0.3943\nEpoch 3, Batch 1140/2240, Loss: 0.5997\nEpoch 3, Batch 1150/2240, Loss: 0.2537\nEpoch 3, Batch 1160/2240, Loss: 0.3966\nEpoch 3, Batch 1170/2240, Loss: 0.1780\nEpoch 3, Batch 1180/2240, Loss: 0.6399\nEpoch 3, Batch 1190/2240, Loss: 1.7888\nEpoch 3, Batch 1200/2240, Loss: 0.3605\nEpoch 3, Batch 1210/2240, Loss: 0.1421\nEpoch 3, Batch 1220/2240, Loss: 0.2053\nEpoch 3, Batch 1230/2240, Loss: 0.2837\nEpoch 3, Batch 1240/2240, Loss: 0.2846\nEpoch 3, Batch 1250/2240, Loss: 0.3993\nEpoch 3, Batch 1260/2240, Loss: 0.1213\nEpoch 3, Batch 1270/2240, Loss: 0.2195\nEpoch 3, Batch 1280/2240, Loss: 0.0760\nEpoch 3, Batch 1290/2240, Loss: 0.3668\nEpoch 3, Batch 1300/2240, Loss: 0.3796\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n  warnings.warn(str(msg))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Batch 1310/2240, Loss: 0.4825\nEpoch 3, Batch 1320/2240, Loss: 0.1855\nEpoch 3, Batch 1330/2240, Loss: 0.4026\nEpoch 3, Batch 1340/2240, Loss: 0.1569\nEpoch 3, Batch 1350/2240, Loss: 0.6221\nEpoch 3, Batch 1360/2240, Loss: 0.0910\nEpoch 3, Batch 1370/2240, Loss: 0.1890\nEpoch 3, Batch 1380/2240, Loss: 0.2166\nEpoch 3, Batch 1390/2240, Loss: 0.4681\nEpoch 3, Batch 1400/2240, Loss: 0.7048\nEpoch 3, Batch 1410/2240, Loss: 0.1573\nEpoch 3, Batch 1420/2240, Loss: 0.3288\nEpoch 3, Batch 1430/2240, Loss: 0.2936\nEpoch 3, Batch 1440/2240, Loss: 0.7950\nEpoch 3, Batch 1450/2240, Loss: 0.3450\nEpoch 3, Batch 1460/2240, Loss: 0.4962\nEpoch 3, Batch 1470/2240, Loss: 0.1857\nEpoch 3, Batch 1480/2240, Loss: 0.6146\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Batch 1490/2240, Loss: 0.3278\nEpoch 3, Batch 1500/2240, Loss: 0.1301\nEpoch 3, Batch 1510/2240, Loss: 0.1668\nEpoch 3, Batch 1520/2240, Loss: 0.2295\nEpoch 3, Batch 1530/2240, Loss: 0.2108\nEpoch 3, Batch 1540/2240, Loss: 0.5655\nEpoch 3, Batch 1550/2240, Loss: 0.1845\nEpoch 3, Batch 1560/2240, Loss: 0.3634\nEpoch 3, Batch 1570/2240, Loss: 0.0767\nEpoch 3, Batch 1580/2240, Loss: 0.3707\nEpoch 3, Batch 1590/2240, Loss: 0.2155\nEpoch 3, Batch 1600/2240, Loss: 0.2571\nEpoch 3, Batch 1610/2240, Loss: 0.2083\nEpoch 3, Batch 1620/2240, Loss: 0.7472\nEpoch 3, Batch 1630/2240, Loss: 0.3211\nEpoch 3, Batch 1640/2240, Loss: 0.2727\nEpoch 3, Batch 1650/2240, Loss: 0.2387\nEpoch 3, Batch 1660/2240, Loss: 0.2172\nEpoch 3, Batch 1670/2240, Loss: 0.2899\nEpoch 3, Batch 1680/2240, Loss: 0.2167\nEpoch 3, Batch 1690/2240, Loss: 0.3408\nEpoch 3, Batch 1700/2240, Loss: 0.3131\nEpoch 3, Batch 1710/2240, Loss: 0.2044\nEpoch 3, Batch 1720/2240, Loss: 0.3474\nEpoch 3, Batch 1730/2240, Loss: 0.4523\nEpoch 3, Batch 1740/2240, Loss: 0.2032\nEpoch 3, Batch 1750/2240, Loss: 0.4130\nEpoch 3, Batch 1760/2240, Loss: 0.1551\nEpoch 3, Batch 1770/2240, Loss: 0.5407\nEpoch 3, Batch 1780/2240, Loss: 0.2648\nEpoch 3, Batch 1790/2240, Loss: 0.7978\nEpoch 3, Batch 1800/2240, Loss: 0.2786\nEpoch 3, Batch 1810/2240, Loss: 0.4047\nEpoch 3, Batch 1820/2240, Loss: 0.1455\nEpoch 3, Batch 1830/2240, Loss: 0.3229\nEpoch 3, Batch 1840/2240, Loss: 0.6472\nEpoch 3, Batch 1850/2240, Loss: 0.4871\nEpoch 3, Batch 1860/2240, Loss: 0.2947\nEpoch 3, Batch 1870/2240, Loss: 0.4155\nEpoch 3, Batch 1880/2240, Loss: 0.2055\nEpoch 3, Batch 1890/2240, Loss: 0.3451\nEpoch 3, Batch 1900/2240, Loss: 0.3114\nEpoch 3, Batch 1910/2240, Loss: 0.3232\nEpoch 3, Batch 1920/2240, Loss: 0.3058\nEpoch 3, Batch 1930/2240, Loss: 0.5000\nEpoch 3, Batch 1940/2240, Loss: 0.4409\nEpoch 3, Batch 1950/2240, Loss: 0.3245\nEpoch 3, Batch 1960/2240, Loss: 0.2560\nEpoch 3, Batch 1970/2240, Loss: 0.6002\nEpoch 3, Batch 1980/2240, Loss: 0.1068\nEpoch 3, Batch 1990/2240, Loss: 0.2764\nEpoch 3, Batch 2000/2240, Loss: 0.2921\nEpoch 3, Batch 2010/2240, Loss: 0.0595\nEpoch 3, Batch 2020/2240, Loss: 0.4596\nEpoch 3, Batch 2030/2240, Loss: 0.5009\nEpoch 3, Batch 2040/2240, Loss: 0.2907\nEpoch 3, Batch 2050/2240, Loss: 0.2863\nEpoch 3, Batch 2060/2240, Loss: 0.1626\nEpoch 3, Batch 2070/2240, Loss: 0.2535\nEpoch 3, Batch 2080/2240, Loss: 0.5731\nEpoch 3, Batch 2090/2240, Loss: 0.3200\nEpoch 3, Batch 2100/2240, Loss: 0.1842\nEpoch 3, Batch 2110/2240, Loss: 0.1278\nEpoch 3, Batch 2120/2240, Loss: 0.0531\nEpoch 3, Batch 2130/2240, Loss: 0.3445\nEpoch 3, Batch 2140/2240, Loss: 0.6746\nEpoch 3, Batch 2150/2240, Loss: 0.1686\nEpoch 3, Batch 2160/2240, Loss: 0.4522\nEpoch 3, Batch 2170/2240, Loss: 0.1089\nEpoch 3, Batch 2180/2240, Loss: 0.2667\nEpoch 3, Batch 2190/2240, Loss: 0.2121\nEpoch 3, Batch 2200/2240, Loss: 0.0526\nEpoch 3, Batch 2210/2240, Loss: 0.2541\nEpoch 3, Batch 2220/2240, Loss: 0.2721\nEpoch 3, Batch 2230/2240, Loss: 0.2711\nEpoch 3, Batch 2240/2240, Loss: 0.1379\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Train Loss: 0.3301, Train Acc: 0.8458, Valid Loss: 0.3997, Valid Acc: 0.8102\nSaved model with validation loss: 0.3997\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Batch 10/2240, Loss: 0.1260\nEpoch 4, Batch 20/2240, Loss: 0.1555\nEpoch 4, Batch 30/2240, Loss: 0.1302\nEpoch 4, Batch 40/2240, Loss: 0.0749\nEpoch 4, Batch 50/2240, Loss: 0.1068\nEpoch 4, Batch 60/2240, Loss: 0.2029\nEpoch 4, Batch 70/2240, Loss: 0.5744\nEpoch 4, Batch 80/2240, Loss: 0.2515\nEpoch 4, Batch 90/2240, Loss: 0.5695\nEpoch 4, Batch 100/2240, Loss: 0.2385\nEpoch 4, Batch 110/2240, Loss: 0.0345\nEpoch 4, Batch 120/2240, Loss: 0.1532\nEpoch 4, Batch 130/2240, Loss: 0.1842\nEpoch 4, Batch 140/2240, Loss: 0.3055\nEpoch 4, Batch 150/2240, Loss: 0.3773\nEpoch 4, Batch 160/2240, Loss: 0.1084\nEpoch 4, Batch 170/2240, Loss: 0.2448\nEpoch 4, Batch 180/2240, Loss: 0.1776\nEpoch 4, Batch 190/2240, Loss: 0.2084\nEpoch 4, Batch 200/2240, Loss: 0.3336\nEpoch 4, Batch 210/2240, Loss: 0.3606\nEpoch 4, Batch 220/2240, Loss: 0.3105\nEpoch 4, Batch 230/2240, Loss: 0.0968\nEpoch 4, Batch 240/2240, Loss: 0.1640\nEpoch 4, Batch 250/2240, Loss: 0.0692\nEpoch 4, Batch 260/2240, Loss: 0.4698\nEpoch 4, Batch 270/2240, Loss: 0.1315\nEpoch 4, Batch 280/2240, Loss: 0.4789\nEpoch 4, Batch 290/2240, Loss: 0.3012\nEpoch 4, Batch 300/2240, Loss: 0.1477\nEpoch 4, Batch 310/2240, Loss: 0.2598\nEpoch 4, Batch 320/2240, Loss: 0.3491\nEpoch 4, Batch 330/2240, Loss: 0.0876\nEpoch 4, Batch 340/2240, Loss: 0.1289\nEpoch 4, Batch 350/2240, Loss: 0.0633\nEpoch 4, Batch 360/2240, Loss: 0.2408\nEpoch 4, Batch 370/2240, Loss: 0.1594\nEpoch 4, Batch 380/2240, Loss: 0.2901\nEpoch 4, Batch 390/2240, Loss: 0.1143\nEpoch 4, Batch 400/2240, Loss: 0.2735\nEpoch 4, Batch 410/2240, Loss: 0.5821\nEpoch 4, Batch 420/2240, Loss: 0.2058\nEpoch 4, Batch 430/2240, Loss: 0.1412\nEpoch 4, Batch 440/2240, Loss: 0.0172\nEpoch 4, Batch 450/2240, Loss: 0.1501\nEpoch 4, Batch 460/2240, Loss: 0.4101\nEpoch 4, Batch 470/2240, Loss: 0.3599\nEpoch 4, Batch 480/2240, Loss: 0.0440\nEpoch 4, Batch 490/2240, Loss: 0.2255\nEpoch 4, Batch 500/2240, Loss: 0.4114\nEpoch 4, Batch 510/2240, Loss: 0.0234\nEpoch 4, Batch 520/2240, Loss: 0.6365\nEpoch 4, Batch 530/2240, Loss: 0.1140\nEpoch 4, Batch 540/2240, Loss: 0.0782\nEpoch 4, Batch 550/2240, Loss: 0.1344\nEpoch 4, Batch 560/2240, Loss: 0.2730\nEpoch 4, Batch 570/2240, Loss: 0.2254\nEpoch 4, Batch 580/2240, Loss: 0.4675\nEpoch 4, Batch 590/2240, Loss: 0.5129\nEpoch 4, Batch 600/2240, Loss: 0.3033\nEpoch 4, Batch 610/2240, Loss: 0.3213\nEpoch 4, Batch 620/2240, Loss: 0.2498\nEpoch 4, Batch 630/2240, Loss: 0.3807\nEpoch 4, Batch 640/2240, Loss: 0.1241\nEpoch 4, Batch 650/2240, Loss: 0.1913\nEpoch 4, Batch 660/2240, Loss: 0.4833\nEpoch 4, Batch 670/2240, Loss: 0.2030\nEpoch 4, Batch 680/2240, Loss: 0.5067\nEpoch 4, Batch 690/2240, Loss: 0.7041\nEpoch 4, Batch 700/2240, Loss: 0.0370\nEpoch 4, Batch 710/2240, Loss: 0.1925\nEpoch 4, Batch 720/2240, Loss: 0.3138\nEpoch 4, Batch 730/2240, Loss: 0.5440\nEpoch 4, Batch 740/2240, Loss: 0.2664\nEpoch 4, Batch 750/2240, Loss: 0.1556\nEpoch 4, Batch 760/2240, Loss: 0.4793\nEpoch 4, Batch 770/2240, Loss: 0.3018\nEpoch 4, Batch 780/2240, Loss: 0.1926\nEpoch 4, Batch 790/2240, Loss: 0.3022\nEpoch 4, Batch 800/2240, Loss: 0.0321\nEpoch 4, Batch 810/2240, Loss: 0.2289\nEpoch 4, Batch 820/2240, Loss: 0.1893\nEpoch 4, Batch 830/2240, Loss: 0.1440\nEpoch 4, Batch 840/2240, Loss: 0.1972\nEpoch 4, Batch 850/2240, Loss: 0.3141\nEpoch 4, Batch 860/2240, Loss: 0.4890\nEpoch 4, Batch 870/2240, Loss: 0.2432\nEpoch 4, Batch 880/2240, Loss: 0.1302\nEpoch 4, Batch 890/2240, Loss: 0.1503\nEpoch 4, Batch 900/2240, Loss: 0.1418\nEpoch 4, Batch 910/2240, Loss: 0.0621\nEpoch 4, Batch 920/2240, Loss: 0.9080\nEpoch 4, Batch 930/2240, Loss: 0.1116\nEpoch 4, Batch 940/2240, Loss: 0.3627\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n  warnings.warn(str(msg))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Batch 950/2240, Loss: 0.2484\nEpoch 4, Batch 960/2240, Loss: 0.1275\nEpoch 4, Batch 970/2240, Loss: 0.1681\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Batch 980/2240, Loss: 0.1096\nEpoch 4, Batch 990/2240, Loss: 0.5306\nEpoch 4, Batch 1000/2240, Loss: 0.2748\nEpoch 4, Batch 1010/2240, Loss: 0.0955\nEpoch 4, Batch 1020/2240, Loss: 0.2490\nEpoch 4, Batch 1030/2240, Loss: 0.1764\nEpoch 4, Batch 1040/2240, Loss: 0.9082\nEpoch 4, Batch 1050/2240, Loss: 0.3553\nEpoch 4, Batch 1060/2240, Loss: 0.4079\nEpoch 4, Batch 1070/2240, Loss: 0.0590\nEpoch 4, Batch 1080/2240, Loss: 0.1621\nEpoch 4, Batch 1090/2240, Loss: 0.0554\nEpoch 4, Batch 1100/2240, Loss: 0.1891\nEpoch 4, Batch 1110/2240, Loss: 0.3588\nEpoch 4, Batch 1120/2240, Loss: 0.3173\nEpoch 4, Batch 1130/2240, Loss: 0.4483\nEpoch 4, Batch 1140/2240, Loss: 0.1100\nEpoch 4, Batch 1150/2240, Loss: 0.5388\nEpoch 4, Batch 1160/2240, Loss: 0.0197\nEpoch 4, Batch 1170/2240, Loss: 0.2633\nEpoch 4, Batch 1180/2240, Loss: 0.0570\nEpoch 4, Batch 1190/2240, Loss: 0.3175\nEpoch 4, Batch 1200/2240, Loss: 0.1701\nEpoch 4, Batch 1210/2240, Loss: 0.0752\nEpoch 4, Batch 1220/2240, Loss: 0.3800\nEpoch 4, Batch 1230/2240, Loss: 0.5529\nEpoch 4, Batch 1240/2240, Loss: 0.2388\nEpoch 4, Batch 1250/2240, Loss: 0.1162\nEpoch 4, Batch 1260/2240, Loss: 0.0759\nEpoch 4, Batch 1270/2240, Loss: 0.1455\nEpoch 4, Batch 1280/2240, Loss: 0.2492\nEpoch 4, Batch 1290/2240, Loss: 0.1624\nEpoch 4, Batch 1300/2240, Loss: 0.1766\nEpoch 4, Batch 1310/2240, Loss: 0.4164\nEpoch 4, Batch 1320/2240, Loss: 0.5100\nEpoch 4, Batch 1330/2240, Loss: 0.1318\nEpoch 4, Batch 1340/2240, Loss: 0.1817\nEpoch 4, Batch 1350/2240, Loss: 0.2994\nEpoch 4, Batch 1360/2240, Loss: 0.2091\nEpoch 4, Batch 1370/2240, Loss: 0.1966\nEpoch 4, Batch 1380/2240, Loss: 0.1267\nEpoch 4, Batch 1390/2240, Loss: 0.1740\nEpoch 4, Batch 1400/2240, Loss: 0.2040\nEpoch 4, Batch 1410/2240, Loss: 0.3221\nEpoch 4, Batch 1420/2240, Loss: 0.2331\nEpoch 4, Batch 1430/2240, Loss: 0.1038\nEpoch 4, Batch 1440/2240, Loss: 0.1400\nEpoch 4, Batch 1450/2240, Loss: 0.0170\nEpoch 4, Batch 1460/2240, Loss: 0.2285\nEpoch 4, Batch 1470/2240, Loss: 0.0643\nEpoch 4, Batch 1480/2240, Loss: 0.0911\nEpoch 4, Batch 1490/2240, Loss: 0.1065\nEpoch 4, Batch 1500/2240, Loss: 0.0380\nEpoch 4, Batch 1510/2240, Loss: 0.0535\nEpoch 4, Batch 1520/2240, Loss: 0.1114\nEpoch 4, Batch 1530/2240, Loss: 0.1938\nEpoch 4, Batch 1540/2240, Loss: 0.1613\nEpoch 4, Batch 1550/2240, Loss: 0.0746\nEpoch 4, Batch 1560/2240, Loss: 0.3861\nEpoch 4, Batch 1570/2240, Loss: 0.3330\nEpoch 4, Batch 1580/2240, Loss: 0.1001\nEpoch 4, Batch 1590/2240, Loss: 0.0721\nEpoch 4, Batch 1600/2240, Loss: 0.2179\nEpoch 4, Batch 1610/2240, Loss: 0.1788\nEpoch 4, Batch 1620/2240, Loss: 0.0973\nEpoch 4, Batch 1630/2240, Loss: 0.0221\nEpoch 4, Batch 1640/2240, Loss: 0.5746\nEpoch 4, Batch 1650/2240, Loss: 0.2824\nEpoch 4, Batch 1660/2240, Loss: 0.0676\nEpoch 4, Batch 1670/2240, Loss: 0.3252\nEpoch 4, Batch 1680/2240, Loss: 0.1318\nEpoch 4, Batch 1690/2240, Loss: 0.1548\nEpoch 4, Batch 1700/2240, Loss: 0.2018\nEpoch 4, Batch 1710/2240, Loss: 0.3518\nEpoch 4, Batch 1720/2240, Loss: 0.0638\nEpoch 4, Batch 1730/2240, Loss: 0.1132\nEpoch 4, Batch 1740/2240, Loss: 0.7775\nEpoch 4, Batch 1750/2240, Loss: 0.2623\nEpoch 4, Batch 1760/2240, Loss: 0.0724\nEpoch 4, Batch 1770/2240, Loss: 0.1955\nEpoch 4, Batch 1780/2240, Loss: 0.3267\nEpoch 4, Batch 1790/2240, Loss: 0.2361\nEpoch 4, Batch 1800/2240, Loss: 0.3465\nEpoch 4, Batch 1810/2240, Loss: 0.2261\nEpoch 4, Batch 1820/2240, Loss: 0.2297\nEpoch 4, Batch 1830/2240, Loss: 0.3309\nEpoch 4, Batch 1840/2240, Loss: 0.0726\nEpoch 4, Batch 1850/2240, Loss: 0.2585\nEpoch 4, Batch 1860/2240, Loss: 0.1131\nEpoch 4, Batch 1870/2240, Loss: 0.4399\nEpoch 4, Batch 1880/2240, Loss: 0.0702\nEpoch 4, Batch 1890/2240, Loss: 0.4411\nEpoch 4, Batch 1900/2240, Loss: 0.2135\nEpoch 4, Batch 1910/2240, Loss: 0.2546\nEpoch 4, Batch 1920/2240, Loss: 0.4504\nEpoch 4, Batch 1930/2240, Loss: 0.1049\nEpoch 4, Batch 1940/2240, Loss: 0.5856\nEpoch 4, Batch 1950/2240, Loss: 0.6826\nEpoch 4, Batch 1960/2240, Loss: 0.3808\nEpoch 4, Batch 1970/2240, Loss: 0.1739\nEpoch 4, Batch 1980/2240, Loss: 0.1919\nEpoch 4, Batch 1990/2240, Loss: 0.3184\nEpoch 4, Batch 2000/2240, Loss: 0.3560\nEpoch 4, Batch 2010/2240, Loss: 0.0933\nEpoch 4, Batch 2020/2240, Loss: 0.1030\nEpoch 4, Batch 2030/2240, Loss: 0.1832\nEpoch 4, Batch 2040/2240, Loss: 0.1770\nEpoch 4, Batch 2050/2240, Loss: 0.3768\nEpoch 4, Batch 2060/2240, Loss: 0.3746\nEpoch 4, Batch 2070/2240, Loss: 0.1067\nEpoch 4, Batch 2080/2240, Loss: 0.1167\nEpoch 4, Batch 2090/2240, Loss: 0.3198\nEpoch 4, Batch 2100/2240, Loss: 0.6045\nEpoch 4, Batch 2110/2240, Loss: 0.5250\nEpoch 4, Batch 2120/2240, Loss: 0.3170\nEpoch 4, Batch 2130/2240, Loss: 0.2632\nEpoch 4, Batch 2140/2240, Loss: 0.1630\nEpoch 4, Batch 2150/2240, Loss: 0.1062\nEpoch 4, Batch 2160/2240, Loss: 0.1078\nEpoch 4, Batch 2170/2240, Loss: 0.0949\nEpoch 4, Batch 2180/2240, Loss: 0.3950\nEpoch 4, Batch 2190/2240, Loss: 0.0864\nEpoch 4, Batch 2200/2240, Loss: 0.0508\nEpoch 4, Batch 2210/2240, Loss: 0.1340\nEpoch 4, Batch 2220/2240, Loss: 0.1900\nEpoch 4, Batch 2230/2240, Loss: 0.0213\nEpoch 4, Batch 2240/2240, Loss: 0.1596\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Train Loss: 0.2534, Train Acc: 0.8876, Valid Loss: 0.4368, Valid Acc: 0.8049\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Batch 10/2240, Loss: 0.1487\nEpoch 5, Batch 20/2240, Loss: 0.0905\nEpoch 5, Batch 30/2240, Loss: 0.0607\nEpoch 5, Batch 40/2240, Loss: 0.1146\nEpoch 5, Batch 50/2240, Loss: 0.0632\nEpoch 5, Batch 60/2240, Loss: 0.0353\nEpoch 5, Batch 70/2240, Loss: 0.0603\nEpoch 5, Batch 80/2240, Loss: 0.0445\nEpoch 5, Batch 90/2240, Loss: 0.0948\nEpoch 5, Batch 100/2240, Loss: 0.2211\nEpoch 5, Batch 110/2240, Loss: 0.1079\nEpoch 5, Batch 120/2240, Loss: 0.0552\nEpoch 5, Batch 130/2240, Loss: 0.1098\nEpoch 5, Batch 140/2240, Loss: 0.0665\nEpoch 5, Batch 150/2240, Loss: 0.1835\nEpoch 5, Batch 160/2240, Loss: 0.1063\nEpoch 5, Batch 170/2240, Loss: 0.0673\nEpoch 5, Batch 180/2240, Loss: 0.2492\nEpoch 5, Batch 190/2240, Loss: 0.1161\nEpoch 5, Batch 200/2240, Loss: 0.1069\nEpoch 5, Batch 210/2240, Loss: 0.2844\nEpoch 5, Batch 220/2240, Loss: 0.1939\nEpoch 5, Batch 230/2240, Loss: 0.0820\nEpoch 5, Batch 240/2240, Loss: 0.0748\nEpoch 5, Batch 250/2240, Loss: 0.1096\nEpoch 5, Batch 260/2240, Loss: 0.5847\nEpoch 5, Batch 270/2240, Loss: 0.0451\nEpoch 5, Batch 280/2240, Loss: 0.0743\nEpoch 5, Batch 290/2240, Loss: 0.0322\nEpoch 5, Batch 300/2240, Loss: 0.0409\nEpoch 5, Batch 310/2240, Loss: 0.0950\nEpoch 5, Batch 320/2240, Loss: 0.0700\nEpoch 5, Batch 330/2240, Loss: 0.2614\nEpoch 5, Batch 340/2240, Loss: 0.0222\nEpoch 5, Batch 350/2240, Loss: 0.2182\nEpoch 5, Batch 360/2240, Loss: 0.2324\nEpoch 5, Batch 370/2240, Loss: 0.1909\nEpoch 5, Batch 380/2240, Loss: 0.7006\nEpoch 5, Batch 390/2240, Loss: 0.0598\nEpoch 5, Batch 400/2240, Loss: 0.3949\nEpoch 5, Batch 410/2240, Loss: 0.4035\nEpoch 5, Batch 420/2240, Loss: 0.0082\nEpoch 5, Batch 430/2240, Loss: 0.1095\nEpoch 5, Batch 440/2240, Loss: 0.4012\nEpoch 5, Batch 450/2240, Loss: 0.2980\nEpoch 5, Batch 460/2240, Loss: 0.2381\nEpoch 5, Batch 470/2240, Loss: 0.0348\nEpoch 5, Batch 480/2240, Loss: 0.0796\nEpoch 5, Batch 490/2240, Loss: 0.1367\nEpoch 5, Batch 500/2240, Loss: 0.0592\nEpoch 5, Batch 510/2240, Loss: 0.1171\nEpoch 5, Batch 520/2240, Loss: 0.0860\nEpoch 5, Batch 530/2240, Loss: 0.0439\nEpoch 5, Batch 540/2240, Loss: 0.2455\nEpoch 5, Batch 550/2240, Loss: 0.2239\nEpoch 5, Batch 560/2240, Loss: 0.1762\nEpoch 5, Batch 570/2240, Loss: 0.0819\nEpoch 5, Batch 580/2240, Loss: 0.1309\nEpoch 5, Batch 590/2240, Loss: 0.3930\nEpoch 5, Batch 600/2240, Loss: 0.0657\nEpoch 5, Batch 610/2240, Loss: 0.0481\nEpoch 5, Batch 620/2240, Loss: 0.2058\nEpoch 5, Batch 630/2240, Loss: 0.0375\nEpoch 5, Batch 640/2240, Loss: 0.1632\nEpoch 5, Batch 650/2240, Loss: 0.0602\nEpoch 5, Batch 660/2240, Loss: 0.3575\nEpoch 5, Batch 670/2240, Loss: 0.3108\nEpoch 5, Batch 680/2240, Loss: 0.2950\nEpoch 5, Batch 690/2240, Loss: 0.0543\nEpoch 5, Batch 700/2240, Loss: 0.1091\nEpoch 5, Batch 710/2240, Loss: 0.1082\nEpoch 5, Batch 720/2240, Loss: 0.0833\nEpoch 5, Batch 730/2240, Loss: 0.0665\nEpoch 5, Batch 740/2240, Loss: 0.2790\nEpoch 5, Batch 750/2240, Loss: 0.1398\nEpoch 5, Batch 760/2240, Loss: 0.1345\nEpoch 5, Batch 770/2240, Loss: 0.1099\nEpoch 5, Batch 780/2240, Loss: 0.0676\nEpoch 5, Batch 790/2240, Loss: 0.1168\nEpoch 5, Batch 800/2240, Loss: 0.0183\nEpoch 5, Batch 810/2240, Loss: 0.0616\nEpoch 5, Batch 820/2240, Loss: 0.0658\nEpoch 5, Batch 830/2240, Loss: 0.1108\nEpoch 5, Batch 840/2240, Loss: 0.0360\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Batch 850/2240, Loss: 0.2565\nEpoch 5, Batch 860/2240, Loss: 0.2470\nEpoch 5, Batch 870/2240, Loss: 0.2692\nEpoch 5, Batch 880/2240, Loss: 1.0967\nEpoch 5, Batch 890/2240, Loss: 0.1085\nEpoch 5, Batch 900/2240, Loss: 0.2103\nEpoch 5, Batch 910/2240, Loss: 0.3245\nEpoch 5, Batch 920/2240, Loss: 0.1876\nEpoch 5, Batch 930/2240, Loss: 0.1292\nEpoch 5, Batch 940/2240, Loss: 0.0019\nEpoch 5, Batch 950/2240, Loss: 0.2493\nEpoch 5, Batch 960/2240, Loss: 0.1449\nEpoch 5, Batch 970/2240, Loss: 0.2734\nEpoch 5, Batch 980/2240, Loss: 0.0516\nEpoch 5, Batch 990/2240, Loss: 0.2169\nEpoch 5, Batch 1000/2240, Loss: 0.0373\nEpoch 5, Batch 1010/2240, Loss: 0.1416\nEpoch 5, Batch 1020/2240, Loss: 0.1994\nEpoch 5, Batch 1030/2240, Loss: 0.1919\nEpoch 5, Batch 1040/2240, Loss: 0.2883\nEpoch 5, Batch 1050/2240, Loss: 0.0650\nEpoch 5, Batch 1060/2240, Loss: 0.0159\nEpoch 5, Batch 1070/2240, Loss: 0.0426\nEpoch 5, Batch 1080/2240, Loss: 0.1338\nEpoch 5, Batch 1090/2240, Loss: 0.4278\nEpoch 5, Batch 1100/2240, Loss: 0.4631\nEpoch 5, Batch 1110/2240, Loss: 0.0578\nEpoch 5, Batch 1120/2240, Loss: 0.0785\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n  warnings.warn(str(msg))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Batch 1130/2240, Loss: 0.0593\nEpoch 5, Batch 1140/2240, Loss: 0.0191\nEpoch 5, Batch 1150/2240, Loss: 0.0889\nEpoch 5, Batch 1160/2240, Loss: 0.4541\nEpoch 5, Batch 1170/2240, Loss: 0.0218\nEpoch 5, Batch 1180/2240, Loss: 0.0888\nEpoch 5, Batch 1190/2240, Loss: 0.1142\nEpoch 5, Batch 1200/2240, Loss: 0.0388\nEpoch 5, Batch 1210/2240, Loss: 0.0856\nEpoch 5, Batch 1220/2240, Loss: 0.0719\nEpoch 5, Batch 1230/2240, Loss: 0.2120\nEpoch 5, Batch 1240/2240, Loss: 0.4149\nEpoch 5, Batch 1250/2240, Loss: 0.0593\nEpoch 5, Batch 1260/2240, Loss: 0.2407\nEpoch 5, Batch 1270/2240, Loss: 0.1444\nEpoch 5, Batch 1280/2240, Loss: 0.0233\nEpoch 5, Batch 1290/2240, Loss: 0.4111\nEpoch 5, Batch 1300/2240, Loss: 0.1369\nEpoch 5, Batch 1310/2240, Loss: 0.2816\nEpoch 5, Batch 1320/2240, Loss: 0.2820\nEpoch 5, Batch 1330/2240, Loss: 0.3685\nEpoch 5, Batch 1340/2240, Loss: 0.0426\nEpoch 5, Batch 1350/2240, Loss: 0.3189\nEpoch 5, Batch 1360/2240, Loss: 0.2388\nEpoch 5, Batch 1370/2240, Loss: 0.1570\nEpoch 5, Batch 1380/2240, Loss: 0.0992\nEpoch 5, Batch 1390/2240, Loss: 0.1231\nEpoch 5, Batch 1400/2240, Loss: 0.1081\nEpoch 5, Batch 1410/2240, Loss: 0.0472\nEpoch 5, Batch 1420/2240, Loss: 0.0608\nEpoch 5, Batch 1430/2240, Loss: 0.1026\nEpoch 5, Batch 1440/2240, Loss: 0.1577\nEpoch 5, Batch 1450/2240, Loss: 0.0506\nEpoch 5, Batch 1460/2240, Loss: 0.0222\nEpoch 5, Batch 1470/2240, Loss: 0.0170\nEpoch 5, Batch 1480/2240, Loss: 0.0671\nEpoch 5, Batch 1490/2240, Loss: 0.1376\nEpoch 5, Batch 1500/2240, Loss: 0.0810\nEpoch 5, Batch 1510/2240, Loss: 0.0227\nEpoch 5, Batch 1520/2240, Loss: 0.3480\nEpoch 5, Batch 1530/2240, Loss: 0.0759\nEpoch 5, Batch 1540/2240, Loss: 0.1655\nEpoch 5, Batch 1550/2240, Loss: 0.2023\nEpoch 5, Batch 1560/2240, Loss: 0.0396\nEpoch 5, Batch 1570/2240, Loss: 0.0213\nEpoch 5, Batch 1580/2240, Loss: 0.0576\nEpoch 5, Batch 1590/2240, Loss: 0.2026\nEpoch 5, Batch 1600/2240, Loss: 0.6290\nEpoch 5, Batch 1610/2240, Loss: 0.0335\nEpoch 5, Batch 1620/2240, Loss: 0.0588\nEpoch 5, Batch 1630/2240, Loss: 0.0616\nEpoch 5, Batch 1640/2240, Loss: 0.1771\nEpoch 5, Batch 1650/2240, Loss: 0.0861\nEpoch 5, Batch 1660/2240, Loss: 0.2346\nEpoch 5, Batch 1670/2240, Loss: 0.1232\nEpoch 5, Batch 1680/2240, Loss: 0.2938\nEpoch 5, Batch 1690/2240, Loss: 0.1212\nEpoch 5, Batch 1700/2240, Loss: 0.1220\nEpoch 5, Batch 1710/2240, Loss: 0.3341\nEpoch 5, Batch 1720/2240, Loss: 0.1573\nEpoch 5, Batch 1730/2240, Loss: 0.0885\nEpoch 5, Batch 1740/2240, Loss: 0.0138\nEpoch 5, Batch 1750/2240, Loss: 0.2110\nEpoch 5, Batch 1760/2240, Loss: 0.0816\nEpoch 5, Batch 1770/2240, Loss: 0.0962\nEpoch 5, Batch 1780/2240, Loss: 0.3230\nEpoch 5, Batch 1790/2240, Loss: 0.0173\nEpoch 5, Batch 1800/2240, Loss: 0.1772\nEpoch 5, Batch 1810/2240, Loss: 0.0562\nEpoch 5, Batch 1820/2240, Loss: 0.0257\nEpoch 5, Batch 1830/2240, Loss: 0.1957\nEpoch 5, Batch 1840/2240, Loss: 0.2981\nEpoch 5, Batch 1850/2240, Loss: 0.1829\nEpoch 5, Batch 1860/2240, Loss: 0.1157\nEpoch 5, Batch 1870/2240, Loss: 0.0755\nEpoch 5, Batch 1880/2240, Loss: 0.2261\nEpoch 5, Batch 1890/2240, Loss: 0.1344\nEpoch 5, Batch 1900/2240, Loss: 0.2053\nEpoch 5, Batch 1910/2240, Loss: 0.4240\nEpoch 5, Batch 1920/2240, Loss: 0.2255\nEpoch 5, Batch 1930/2240, Loss: 0.0982\nEpoch 5, Batch 1940/2240, Loss: 0.1756\nEpoch 5, Batch 1950/2240, Loss: 0.0352\nEpoch 5, Batch 1960/2240, Loss: 0.0406\nEpoch 5, Batch 1970/2240, Loss: 0.1232\nEpoch 5, Batch 1980/2240, Loss: 0.0615\nEpoch 5, Batch 1990/2240, Loss: 0.0201\nEpoch 5, Batch 2000/2240, Loss: 0.0496\nEpoch 5, Batch 2010/2240, Loss: 0.2042\nEpoch 5, Batch 2020/2240, Loss: 0.2807\nEpoch 5, Batch 2030/2240, Loss: 0.0466\nEpoch 5, Batch 2040/2240, Loss: 0.0309\nEpoch 5, Batch 2050/2240, Loss: 0.1762\nEpoch 5, Batch 2060/2240, Loss: 0.4995\nEpoch 5, Batch 2070/2240, Loss: 0.0733\nEpoch 5, Batch 2080/2240, Loss: 0.0978\nEpoch 5, Batch 2090/2240, Loss: 0.2529\nEpoch 5, Batch 2100/2240, Loss: 0.3401\nEpoch 5, Batch 2110/2240, Loss: 0.0764\nEpoch 5, Batch 2120/2240, Loss: 0.0259\nEpoch 5, Batch 2130/2240, Loss: 0.1153\nEpoch 5, Batch 2140/2240, Loss: 0.0511\nEpoch 5, Batch 2150/2240, Loss: 0.0257\nEpoch 5, Batch 2160/2240, Loss: 0.0384\nEpoch 5, Batch 2170/2240, Loss: 0.0201\nEpoch 5, Batch 2180/2240, Loss: 0.0822\nEpoch 5, Batch 2190/2240, Loss: 0.0618\nEpoch 5, Batch 2200/2240, Loss: 0.2572\nEpoch 5, Batch 2210/2240, Loss: 0.0094\nEpoch 5, Batch 2220/2240, Loss: 0.5266\nEpoch 5, Batch 2230/2240, Loss: 0.1548\nEpoch 5, Batch 2240/2240, Loss: 0.0236\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Train Loss: 0.1632, Train Acc: 0.9353, Valid Loss: 0.5999, Valid Acc: 0.8065\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Batch 10/2240, Loss: 0.0092\nEpoch 6, Batch 20/2240, Loss: 0.1116\nEpoch 6, Batch 30/2240, Loss: 0.1706\nEpoch 6, Batch 40/2240, Loss: 0.0274\nEpoch 6, Batch 50/2240, Loss: 0.0020\nEpoch 6, Batch 60/2240, Loss: 0.0128\nEpoch 6, Batch 70/2240, Loss: 0.0113\nEpoch 6, Batch 80/2240, Loss: 0.4712\nEpoch 6, Batch 90/2240, Loss: 0.0048\nEpoch 6, Batch 100/2240, Loss: 0.0368\nEpoch 6, Batch 110/2240, Loss: 0.0952\nEpoch 6, Batch 120/2240, Loss: 0.0397\nEpoch 6, Batch 130/2240, Loss: 0.0311\nEpoch 6, Batch 140/2240, Loss: 0.0104\nEpoch 6, Batch 150/2240, Loss: 0.0470\nEpoch 6, Batch 160/2240, Loss: 0.1784\nEpoch 6, Batch 170/2240, Loss: 0.7954\nEpoch 6, Batch 180/2240, Loss: 0.0323\nEpoch 6, Batch 190/2240, Loss: 0.0356\nEpoch 6, Batch 200/2240, Loss: 0.0729\nEpoch 6, Batch 210/2240, Loss: 0.0360\nEpoch 6, Batch 220/2240, Loss: 0.0470\nEpoch 6, Batch 230/2240, Loss: 0.0325\nEpoch 6, Batch 240/2240, Loss: 0.1311\nEpoch 6, Batch 250/2240, Loss: 0.0431\nEpoch 6, Batch 260/2240, Loss: 0.1451\nEpoch 6, Batch 270/2240, Loss: 0.0059\nEpoch 6, Batch 280/2240, Loss: 0.0253\nEpoch 6, Batch 290/2240, Loss: 0.0047\nEpoch 6, Batch 300/2240, Loss: 0.0836\nEpoch 6, Batch 310/2240, Loss: 0.1195\nEpoch 6, Batch 320/2240, Loss: 0.0053\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Batch 330/2240, Loss: 0.2068\nEpoch 6, Batch 340/2240, Loss: 0.1209\nEpoch 6, Batch 350/2240, Loss: 0.3345\nEpoch 6, Batch 360/2240, Loss: 0.1960\nEpoch 6, Batch 370/2240, Loss: 0.0453\nEpoch 6, Batch 380/2240, Loss: 0.0865\nEpoch 6, Batch 390/2240, Loss: 0.0624\nEpoch 6, Batch 400/2240, Loss: 0.0641\nEpoch 6, Batch 410/2240, Loss: 0.0344\nEpoch 6, Batch 420/2240, Loss: 0.0675\nEpoch 6, Batch 430/2240, Loss: 0.3616\nEpoch 6, Batch 440/2240, Loss: 0.2081\nEpoch 6, Batch 450/2240, Loss: 0.0111\nEpoch 6, Batch 460/2240, Loss: 0.1190\nEpoch 6, Batch 470/2240, Loss: 0.0115\nEpoch 6, Batch 480/2240, Loss: 0.1257\nEpoch 6, Batch 490/2240, Loss: 0.0371\nEpoch 6, Batch 500/2240, Loss: 0.1091\nEpoch 6, Batch 510/2240, Loss: 0.0111\nEpoch 6, Batch 520/2240, Loss: 0.0501\nEpoch 6, Batch 530/2240, Loss: 0.0035\nEpoch 6, Batch 540/2240, Loss: 0.0426\nEpoch 6, Batch 550/2240, Loss: 0.5322\nEpoch 6, Batch 560/2240, Loss: 0.0268\nEpoch 6, Batch 570/2240, Loss: 0.0448\nEpoch 6, Batch 580/2240, Loss: 0.0066\nEpoch 6, Batch 590/2240, Loss: 0.3162\nEpoch 6, Batch 600/2240, Loss: 0.0010\nEpoch 6, Batch 610/2240, Loss: 0.0969\nEpoch 6, Batch 620/2240, Loss: 0.1330\nEpoch 6, Batch 630/2240, Loss: 0.0450\nEpoch 6, Batch 640/2240, Loss: 0.0146\nEpoch 6, Batch 650/2240, Loss: 0.0301\nEpoch 6, Batch 660/2240, Loss: 0.0038\nEpoch 6, Batch 670/2240, Loss: 0.2379\nEpoch 6, Batch 680/2240, Loss: 0.0632\nEpoch 6, Batch 690/2240, Loss: 0.1575\nEpoch 6, Batch 700/2240, Loss: 0.0055\nEpoch 6, Batch 710/2240, Loss: 0.0815\nEpoch 6, Batch 720/2240, Loss: 0.1423\nEpoch 6, Batch 730/2240, Loss: 0.0327\nEpoch 6, Batch 740/2240, Loss: 0.0768\nEpoch 6, Batch 750/2240, Loss: 0.0154\nEpoch 6, Batch 760/2240, Loss: 0.0036\nEpoch 6, Batch 770/2240, Loss: 0.0395\nEpoch 6, Batch 780/2240, Loss: 0.1018\nEpoch 6, Batch 790/2240, Loss: 0.0147\nEpoch 6, Batch 800/2240, Loss: 0.0658\nEpoch 6, Batch 810/2240, Loss: 0.0173\nEpoch 6, Batch 820/2240, Loss: 0.0520\nEpoch 6, Batch 830/2240, Loss: 0.0284\nEpoch 6, Batch 840/2240, Loss: 0.0009\nEpoch 6, Batch 850/2240, Loss: 0.0270\nEpoch 6, Batch 860/2240, Loss: 0.0063\nEpoch 6, Batch 870/2240, Loss: 0.0846\nEpoch 6, Batch 880/2240, Loss: 0.0018\nEpoch 6, Batch 890/2240, Loss: 0.0124\nEpoch 6, Batch 900/2240, Loss: 0.1946\nEpoch 6, Batch 910/2240, Loss: 0.0347\nEpoch 6, Batch 920/2240, Loss: 0.0057\nEpoch 6, Batch 930/2240, Loss: 0.1997\nEpoch 6, Batch 940/2240, Loss: 0.0148\nEpoch 6, Batch 950/2240, Loss: 0.0075\nEpoch 6, Batch 960/2240, Loss: 0.0258\nEpoch 6, Batch 970/2240, Loss: 0.0261\nEpoch 6, Batch 980/2240, Loss: 0.4263\nEpoch 6, Batch 990/2240, Loss: 0.0069\nEpoch 6, Batch 1000/2240, Loss: 0.0251\nEpoch 6, Batch 1010/2240, Loss: 0.1186\nEpoch 6, Batch 1020/2240, Loss: 0.0399\nEpoch 6, Batch 1030/2240, Loss: 0.0102\nEpoch 6, Batch 1040/2240, Loss: 0.1264\nEpoch 6, Batch 1050/2240, Loss: 0.0154\nEpoch 6, Batch 1060/2240, Loss: 0.0190\nEpoch 6, Batch 1070/2240, Loss: 0.3675\nEpoch 6, Batch 1080/2240, Loss: 0.1159\nEpoch 6, Batch 1090/2240, Loss: 0.0631\nEpoch 6, Batch 1100/2240, Loss: 0.0470\nEpoch 6, Batch 1110/2240, Loss: 0.0458\nEpoch 6, Batch 1120/2240, Loss: 0.0422\nEpoch 6, Batch 1130/2240, Loss: 0.1409\nEpoch 6, Batch 1140/2240, Loss: 0.2689\nEpoch 6, Batch 1150/2240, Loss: 0.1025\nEpoch 6, Batch 1160/2240, Loss: 0.0853\nEpoch 6, Batch 1170/2240, Loss: 0.0078\nEpoch 6, Batch 1180/2240, Loss: 0.0181\nEpoch 6, Batch 1190/2240, Loss: 0.2885\nEpoch 6, Batch 1200/2240, Loss: 0.1452\nEpoch 6, Batch 1210/2240, Loss: 0.0563\nEpoch 6, Batch 1220/2240, Loss: 0.8710\nEpoch 6, Batch 1230/2240, Loss: 0.0552\nEpoch 6, Batch 1240/2240, Loss: 0.0396\nEpoch 6, Batch 1250/2240, Loss: 0.0520\nEpoch 6, Batch 1260/2240, Loss: 0.3864\nEpoch 6, Batch 1270/2240, Loss: 0.0498\nEpoch 6, Batch 1280/2240, Loss: 0.2929\nEpoch 6, Batch 1290/2240, Loss: 0.1336\nEpoch 6, Batch 1300/2240, Loss: 0.0089\nEpoch 6, Batch 1310/2240, Loss: 0.1390\nEpoch 6, Batch 1320/2240, Loss: 0.3724\nEpoch 6, Batch 1330/2240, Loss: 0.0450\nEpoch 6, Batch 1340/2240, Loss: 0.0772\nEpoch 6, Batch 1350/2240, Loss: 0.0582\nEpoch 6, Batch 1360/2240, Loss: 0.2581\nEpoch 6, Batch 1370/2240, Loss: 0.5997\nEpoch 6, Batch 1380/2240, Loss: 0.1636\nEpoch 6, Batch 1390/2240, Loss: 0.0369\nEpoch 6, Batch 1400/2240, Loss: 0.3590\nEpoch 6, Batch 1410/2240, Loss: 0.0875\nEpoch 6, Batch 1420/2240, Loss: 0.0182\nEpoch 6, Batch 1430/2240, Loss: 0.1476\nEpoch 6, Batch 1440/2240, Loss: 0.4905\nEpoch 6, Batch 1450/2240, Loss: 0.0646\nEpoch 6, Batch 1460/2240, Loss: 0.0471\nEpoch 6, Batch 1470/2240, Loss: 0.0136\nEpoch 6, Batch 1480/2240, Loss: 0.0529\nEpoch 6, Batch 1490/2240, Loss: 0.0430\nEpoch 6, Batch 1500/2240, Loss: 0.0042\nEpoch 6, Batch 1510/2240, Loss: 0.0357\nEpoch 6, Batch 1520/2240, Loss: 0.1970\nEpoch 6, Batch 1530/2240, Loss: 0.0197\nEpoch 6, Batch 1540/2240, Loss: 0.0192\nEpoch 6, Batch 1550/2240, Loss: 0.0134\nEpoch 6, Batch 1560/2240, Loss: 0.0194\nEpoch 6, Batch 1570/2240, Loss: 0.0395\nEpoch 6, Batch 1580/2240, Loss: 0.0781\nEpoch 6, Batch 1590/2240, Loss: 0.0970\nEpoch 6, Batch 1600/2240, Loss: 0.0042\nEpoch 6, Batch 1610/2240, Loss: 0.1114\nEpoch 6, Batch 1620/2240, Loss: 0.4072\nEpoch 6, Batch 1630/2240, Loss: 0.1207\nEpoch 6, Batch 1640/2240, Loss: 0.1231\nEpoch 6, Batch 1650/2240, Loss: 0.0665\nEpoch 6, Batch 1660/2240, Loss: 0.0145\nEpoch 6, Batch 1670/2240, Loss: 0.3067\nEpoch 6, Batch 1680/2240, Loss: 0.0748\nEpoch 6, Batch 1690/2240, Loss: 0.0253\nEpoch 6, Batch 1700/2240, Loss: 0.0102\nEpoch 6, Batch 1710/2240, Loss: 0.1469\nEpoch 6, Batch 1720/2240, Loss: 0.0468\nEpoch 6, Batch 1730/2240, Loss: 0.1799\nEpoch 6, Batch 1740/2240, Loss: 0.1384\nEpoch 6, Batch 1750/2240, Loss: 0.0615\nEpoch 6, Batch 1760/2240, Loss: 0.0382\nEpoch 6, Batch 1770/2240, Loss: 0.0307\nEpoch 6, Batch 1780/2240, Loss: 0.3094\nEpoch 6, Batch 1790/2240, Loss: 0.0124\nEpoch 6, Batch 1800/2240, Loss: 0.0172\nEpoch 6, Batch 1810/2240, Loss: 0.1328\nEpoch 6, Batch 1820/2240, Loss: 0.0415\nEpoch 6, Batch 1830/2240, Loss: 0.0330\nEpoch 6, Batch 1840/2240, Loss: 0.1017\nEpoch 6, Batch 1850/2240, Loss: 0.0561\nEpoch 6, Batch 1860/2240, Loss: 0.0885\nEpoch 6, Batch 1870/2240, Loss: 0.2958\nEpoch 6, Batch 1880/2240, Loss: 0.0242\nEpoch 6, Batch 1890/2240, Loss: 0.1286\nEpoch 6, Batch 1900/2240, Loss: 0.0160\nEpoch 6, Batch 1910/2240, Loss: 0.0030\nEpoch 6, Batch 1920/2240, Loss: 0.0092\nEpoch 6, Batch 1930/2240, Loss: 0.0458\nEpoch 6, Batch 1940/2240, Loss: 0.0050\nEpoch 6, Batch 1950/2240, Loss: 0.0402\nEpoch 6, Batch 1960/2240, Loss: 0.0317\nEpoch 6, Batch 1970/2240, Loss: 0.0100\nEpoch 6, Batch 1980/2240, Loss: 0.2620\nEpoch 6, Batch 1990/2240, Loss: 0.1964\nEpoch 6, Batch 2000/2240, Loss: 0.0161\nEpoch 6, Batch 2010/2240, Loss: 0.2051\nEpoch 6, Batch 2020/2240, Loss: 0.4643\nEpoch 6, Batch 2030/2240, Loss: 0.0897\nEpoch 6, Batch 2040/2240, Loss: 0.0085\nEpoch 6, Batch 2050/2240, Loss: 1.0402\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n  warnings.warn(str(msg))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Batch 2060/2240, Loss: 0.3062\nEpoch 6, Batch 2070/2240, Loss: 0.2353\nEpoch 6, Batch 2080/2240, Loss: 0.0354\nEpoch 6, Batch 2090/2240, Loss: 0.0450\nEpoch 6, Batch 2100/2240, Loss: 0.1702\nEpoch 6, Batch 2110/2240, Loss: 0.0031\nEpoch 6, Batch 2120/2240, Loss: 0.0080\nEpoch 6, Batch 2130/2240, Loss: 0.0279\nEpoch 6, Batch 2140/2240, Loss: 0.0241\nEpoch 6, Batch 2150/2240, Loss: 0.0372\nEpoch 6, Batch 2160/2240, Loss: 0.0704\nEpoch 6, Batch 2170/2240, Loss: 0.2473\nEpoch 6, Batch 2180/2240, Loss: 0.1398\nEpoch 6, Batch 2190/2240, Loss: 0.4925\nEpoch 6, Batch 2200/2240, Loss: 0.0874\nEpoch 6, Batch 2210/2240, Loss: 0.0309\nEpoch 6, Batch 2220/2240, Loss: 0.1788\nEpoch 6, Batch 2230/2240, Loss: 0.0952\nEpoch 6, Batch 2240/2240, Loss: 0.0492\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Train Loss: 0.1047, Train Acc: 0.9611, Valid Loss: 0.6935, Valid Acc: 0.8021\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Batch 10/2240, Loss: 0.0343\nEpoch 7, Batch 20/2240, Loss: 0.0272\nEpoch 7, Batch 30/2240, Loss: 0.0256\nEpoch 7, Batch 40/2240, Loss: 0.0451\nEpoch 7, Batch 50/2240, Loss: 0.4740\nEpoch 7, Batch 60/2240, Loss: 0.0749\nEpoch 7, Batch 70/2240, Loss: 0.0072\nEpoch 7, Batch 80/2240, Loss: 0.0353\nEpoch 7, Batch 90/2240, Loss: 0.0978\nEpoch 7, Batch 100/2240, Loss: 0.0186\nEpoch 7, Batch 110/2240, Loss: 0.0234\nEpoch 7, Batch 120/2240, Loss: 0.0126\nEpoch 7, Batch 130/2240, Loss: 0.0133\nEpoch 7, Batch 140/2240, Loss: 0.0053\nEpoch 7, Batch 150/2240, Loss: 0.0441\nEpoch 7, Batch 160/2240, Loss: 0.4069\nEpoch 7, Batch 170/2240, Loss: 0.0245\nEpoch 7, Batch 180/2240, Loss: 0.0168\nEpoch 7, Batch 190/2240, Loss: 0.0460\nEpoch 7, Batch 200/2240, Loss: 0.0063\nEpoch 7, Batch 210/2240, Loss: 0.0261\nEpoch 7, Batch 220/2240, Loss: 0.0210\nEpoch 7, Batch 230/2240, Loss: 0.0215\nEpoch 7, Batch 240/2240, Loss: 0.0116\nEpoch 7, Batch 250/2240, Loss: 0.0185\nEpoch 7, Batch 260/2240, Loss: 0.0220\nEpoch 7, Batch 270/2240, Loss: 0.0244\nEpoch 7, Batch 280/2240, Loss: 0.0072\nEpoch 7, Batch 290/2240, Loss: 0.1713\nEpoch 7, Batch 300/2240, Loss: 0.0065\nEpoch 7, Batch 310/2240, Loss: 0.0129\nEpoch 7, Batch 320/2240, Loss: 0.0348\nEpoch 7, Batch 330/2240, Loss: 0.0160\nEpoch 7, Batch 340/2240, Loss: 0.0036\nEpoch 7, Batch 350/2240, Loss: 0.0103\nEpoch 7, Batch 360/2240, Loss: 0.0583\nEpoch 7, Batch 370/2240, Loss: 0.1821\nEpoch 7, Batch 380/2240, Loss: 0.3101\nEpoch 7, Batch 390/2240, Loss: 0.1040\nEpoch 7, Batch 400/2240, Loss: 0.0020\nEpoch 7, Batch 410/2240, Loss: 0.0082\nEpoch 7, Batch 420/2240, Loss: 0.0405\nEpoch 7, Batch 430/2240, Loss: 0.0313\nEpoch 7, Batch 440/2240, Loss: 0.0535\nEpoch 7, Batch 450/2240, Loss: 0.0124\nEpoch 7, Batch 460/2240, Loss: 0.0530\nEpoch 7, Batch 470/2240, Loss: 0.1929\nEpoch 7, Batch 480/2240, Loss: 0.1384\nEpoch 7, Batch 490/2240, Loss: 0.0024\nEpoch 7, Batch 500/2240, Loss: 0.0732\nEpoch 7, Batch 510/2240, Loss: 0.0011\nEpoch 7, Batch 520/2240, Loss: 0.2184\nEpoch 7, Batch 530/2240, Loss: 0.0035\nEpoch 7, Batch 540/2240, Loss: 0.0051\nEpoch 7, Batch 550/2240, Loss: 0.0256\nEpoch 7, Batch 560/2240, Loss: 0.1523\nEpoch 7, Batch 570/2240, Loss: 0.0277\nEpoch 7, Batch 580/2240, Loss: 0.0749\nEpoch 7, Batch 590/2240, Loss: 0.0023\nEpoch 7, Batch 600/2240, Loss: 0.0830\nEpoch 7, Batch 610/2240, Loss: 0.0022\nEpoch 7, Batch 620/2240, Loss: 0.0086\nEpoch 7, Batch 630/2240, Loss: 0.0079\nEpoch 7, Batch 640/2240, Loss: 0.0484\nEpoch 7, Batch 650/2240, Loss: 0.0758\nEpoch 7, Batch 660/2240, Loss: 0.0539\nEpoch 7, Batch 670/2240, Loss: 0.0020\nEpoch 7, Batch 680/2240, Loss: 0.0337\nEpoch 7, Batch 690/2240, Loss: 0.0183\nEpoch 7, Batch 700/2240, Loss: 0.0820\nEpoch 7, Batch 710/2240, Loss: 0.2241\nEpoch 7, Batch 720/2240, Loss: 0.0017\nEpoch 7, Batch 730/2240, Loss: 0.0769\nEpoch 7, Batch 740/2240, Loss: 0.0023\nEpoch 7, Batch 750/2240, Loss: 0.0609\nEpoch 7, Batch 760/2240, Loss: 0.1843\nEpoch 7, Batch 770/2240, Loss: 0.0047\nEpoch 7, Batch 780/2240, Loss: 0.2381\nEpoch 7, Batch 790/2240, Loss: 0.1820\nEpoch 7, Batch 800/2240, Loss: 0.0021\nEpoch 7, Batch 810/2240, Loss: 0.0022\nEpoch 7, Batch 820/2240, Loss: 0.3169\nEpoch 7, Batch 830/2240, Loss: 0.0385\nEpoch 7, Batch 840/2240, Loss: 0.0092\nEpoch 7, Batch 850/2240, Loss: 0.0184\nEpoch 7, Batch 860/2240, Loss: 0.0025\nEpoch 7, Batch 870/2240, Loss: 0.0166\nEpoch 7, Batch 880/2240, Loss: 0.1615\nEpoch 7, Batch 890/2240, Loss: 0.0597\nEpoch 7, Batch 900/2240, Loss: 0.2595\nEpoch 7, Batch 910/2240, Loss: 0.0497\nEpoch 7, Batch 920/2240, Loss: 0.1058\nEpoch 7, Batch 930/2240, Loss: 0.0297\nEpoch 7, Batch 940/2240, Loss: 0.0049\nEpoch 7, Batch 950/2240, Loss: 0.0089\nEpoch 7, Batch 960/2240, Loss: 0.0277\nEpoch 7, Batch 970/2240, Loss: 0.0532\nEpoch 7, Batch 980/2240, Loss: 0.0076\nEpoch 7, Batch 990/2240, Loss: 0.0024\nEpoch 7, Batch 1000/2240, Loss: 0.0037\nEpoch 7, Batch 1010/2240, Loss: 0.0874\nEpoch 7, Batch 1020/2240, Loss: 0.0185\nEpoch 7, Batch 1030/2240, Loss: 0.0029\nEpoch 7, Batch 1040/2240, Loss: 0.0106\nEpoch 7, Batch 1050/2240, Loss: 0.0131\nEpoch 7, Batch 1060/2240, Loss: 0.0660\nEpoch 7, Batch 1070/2240, Loss: 0.0113\nEpoch 7, Batch 1080/2240, Loss: 0.0711\nEpoch 7, Batch 1090/2240, Loss: 0.0018\nEpoch 7, Batch 1100/2240, Loss: 0.0001\nEpoch 7, Batch 1110/2240, Loss: 0.2493\nEpoch 7, Batch 1120/2240, Loss: 0.0667\nEpoch 7, Batch 1130/2240, Loss: 0.0022\nEpoch 7, Batch 1140/2240, Loss: 0.0183\nEpoch 7, Batch 1150/2240, Loss: 0.0264\nEpoch 7, Batch 1160/2240, Loss: 0.0036\nEpoch 7, Batch 1170/2240, Loss: 0.0204\nEpoch 7, Batch 1180/2240, Loss: 0.2577\nEpoch 7, Batch 1190/2240, Loss: 0.0124\nEpoch 7, Batch 1200/2240, Loss: 0.0083\nEpoch 7, Batch 1210/2240, Loss: 0.0053\nEpoch 7, Batch 1220/2240, Loss: 0.0134\nEpoch 7, Batch 1230/2240, Loss: 0.0112\nEpoch 7, Batch 1240/2240, Loss: 0.0187\nEpoch 7, Batch 1250/2240, Loss: 0.0455\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n  warnings.warn(str(msg))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Batch 1260/2240, Loss: 0.0351\nEpoch 7, Batch 1270/2240, Loss: 0.0452\nEpoch 7, Batch 1280/2240, Loss: 0.0156\nEpoch 7, Batch 1290/2240, Loss: 0.0329\nEpoch 7, Batch 1300/2240, Loss: 0.0185\nEpoch 7, Batch 1310/2240, Loss: 0.0159\nEpoch 7, Batch 1320/2240, Loss: 0.0081\nEpoch 7, Batch 1330/2240, Loss: 0.0251\nEpoch 7, Batch 1340/2240, Loss: 0.0586\nEpoch 7, Batch 1350/2240, Loss: 0.3678\nEpoch 7, Batch 1360/2240, Loss: 0.0523\nEpoch 7, Batch 1370/2240, Loss: 0.0369\nEpoch 7, Batch 1380/2240, Loss: 0.1494\nEpoch 7, Batch 1390/2240, Loss: 0.1039\nEpoch 7, Batch 1400/2240, Loss: 0.1032\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Batch 1410/2240, Loss: 0.0931\nEpoch 7, Batch 1420/2240, Loss: 0.0075\nEpoch 7, Batch 1430/2240, Loss: 0.0154\nEpoch 7, Batch 1440/2240, Loss: 0.4171\nEpoch 7, Batch 1450/2240, Loss: 0.0358\nEpoch 7, Batch 1460/2240, Loss: 0.2594\nEpoch 7, Batch 1470/2240, Loss: 0.0247\nEpoch 7, Batch 1480/2240, Loss: 0.0641\nEpoch 7, Batch 1490/2240, Loss: 0.0166\nEpoch 7, Batch 1500/2240, Loss: 0.0031\nEpoch 7, Batch 1510/2240, Loss: 0.4376\nEpoch 7, Batch 1520/2240, Loss: 0.0543\nEpoch 7, Batch 1530/2240, Loss: 0.0290\nEpoch 7, Batch 1540/2240, Loss: 0.0300\nEpoch 7, Batch 1550/2240, Loss: 0.0289\nEpoch 7, Batch 1560/2240, Loss: 0.0422\nEpoch 7, Batch 1570/2240, Loss: 0.2055\nEpoch 7, Batch 1580/2240, Loss: 0.0138\nEpoch 7, Batch 1590/2240, Loss: 0.1567\nEpoch 7, Batch 1600/2240, Loss: 0.0393\nEpoch 7, Batch 1610/2240, Loss: 0.0436\nEpoch 7, Batch 1620/2240, Loss: 0.0063\nEpoch 7, Batch 1630/2240, Loss: 0.0507\nEpoch 7, Batch 1640/2240, Loss: 0.0156\nEpoch 7, Batch 1650/2240, Loss: 0.1724\nEpoch 7, Batch 1660/2240, Loss: 0.0544\nEpoch 7, Batch 1670/2240, Loss: 0.0357\nEpoch 7, Batch 1680/2240, Loss: 0.0065\nEpoch 7, Batch 1690/2240, Loss: 0.0132\nEpoch 7, Batch 1700/2240, Loss: 0.2048\nEpoch 7, Batch 1710/2240, Loss: 0.0444\nEpoch 7, Batch 1720/2240, Loss: 0.0015\nEpoch 7, Batch 1730/2240, Loss: 0.0722\nEpoch 7, Batch 1740/2240, Loss: 0.2157\nEpoch 7, Batch 1750/2240, Loss: 0.6153\nEpoch 7, Batch 1760/2240, Loss: 0.1155\nEpoch 7, Batch 1770/2240, Loss: 0.2110\nEpoch 7, Batch 1780/2240, Loss: 0.0504\nEpoch 7, Batch 1790/2240, Loss: 0.0161\nEpoch 7, Batch 1800/2240, Loss: 0.0431\nEpoch 7, Batch 1810/2240, Loss: 0.0012\nEpoch 7, Batch 1820/2240, Loss: 0.0233\nEpoch 7, Batch 1830/2240, Loss: 0.0089\nEpoch 7, Batch 1840/2240, Loss: 0.0039\nEpoch 7, Batch 1850/2240, Loss: 0.8187\nEpoch 7, Batch 1860/2240, Loss: 0.2648\nEpoch 7, Batch 1870/2240, Loss: 0.0224\nEpoch 7, Batch 1880/2240, Loss: 0.0015\nEpoch 7, Batch 1890/2240, Loss: 0.0078\nEpoch 7, Batch 1900/2240, Loss: 0.0598\nEpoch 7, Batch 1910/2240, Loss: 0.0103\nEpoch 7, Batch 1920/2240, Loss: 0.0057\nEpoch 7, Batch 1930/2240, Loss: 0.6047\nEpoch 7, Batch 1940/2240, Loss: 0.0059\nEpoch 7, Batch 1950/2240, Loss: 0.0170\nEpoch 7, Batch 1960/2240, Loss: 0.0244\nEpoch 7, Batch 1970/2240, Loss: 0.0512\nEpoch 7, Batch 1980/2240, Loss: 0.0362\nEpoch 7, Batch 1990/2240, Loss: 0.1333\nEpoch 7, Batch 2000/2240, Loss: 0.0175\nEpoch 7, Batch 2010/2240, Loss: 0.0153\nEpoch 7, Batch 2020/2240, Loss: 0.0053\nEpoch 7, Batch 2030/2240, Loss: 0.0351\nEpoch 7, Batch 2040/2240, Loss: 0.0357\nEpoch 7, Batch 2050/2240, Loss: 0.0342\nEpoch 7, Batch 2060/2240, Loss: 0.1475\nEpoch 7, Batch 2070/2240, Loss: 0.0517\nEpoch 7, Batch 2080/2240, Loss: 0.0160\nEpoch 7, Batch 2090/2240, Loss: 0.0141\nEpoch 7, Batch 2100/2240, Loss: 0.0630\nEpoch 7, Batch 2110/2240, Loss: 0.1608\nEpoch 7, Batch 2120/2240, Loss: 0.0622\nEpoch 7, Batch 2130/2240, Loss: 0.0053\nEpoch 7, Batch 2140/2240, Loss: 0.0100\nEpoch 7, Batch 2150/2240, Loss: 0.0766\nEpoch 7, Batch 2160/2240, Loss: 0.0154\nEpoch 7, Batch 2170/2240, Loss: 0.3103\nEpoch 7, Batch 2180/2240, Loss: 0.0016\nEpoch 7, Batch 2190/2240, Loss: 0.0397\nEpoch 7, Batch 2200/2240, Loss: 0.0379\nEpoch 7, Batch 2210/2240, Loss: 0.0046\nEpoch 7, Batch 2220/2240, Loss: 0.0372\nEpoch 7, Batch 2230/2240, Loss: 0.1801\nEpoch 7, Batch 2240/2240, Loss: 0.0148\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Train Loss: 0.0693, Train Acc: 0.9761, Valid Loss: 0.8241, Valid Acc: 0.7904\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Batch 10/2240, Loss: 0.8730\nEpoch 8, Batch 20/2240, Loss: 0.0576\nEpoch 8, Batch 30/2240, Loss: 0.0249\nEpoch 8, Batch 40/2240, Loss: 0.0188\nEpoch 8, Batch 50/2240, Loss: 0.0036\nEpoch 8, Batch 60/2240, Loss: 0.0023\nEpoch 8, Batch 70/2240, Loss: 0.0226\nEpoch 8, Batch 80/2240, Loss: 0.0273\nEpoch 8, Batch 90/2240, Loss: 0.0830\nEpoch 8, Batch 100/2240, Loss: 0.0152\nEpoch 8, Batch 110/2240, Loss: 0.0149\nEpoch 8, Batch 120/2240, Loss: 0.3243\nEpoch 8, Batch 130/2240, Loss: 0.0219\nEpoch 8, Batch 140/2240, Loss: 0.0309\nEpoch 8, Batch 150/2240, Loss: 0.0023\nEpoch 8, Batch 160/2240, Loss: 0.2131\nEpoch 8, Batch 170/2240, Loss: 0.0251\nEpoch 8, Batch 180/2240, Loss: 0.0643\nEpoch 8, Batch 190/2240, Loss: 0.0029\nEpoch 8, Batch 200/2240, Loss: 0.0020\nEpoch 8, Batch 210/2240, Loss: 0.0160\nEpoch 8, Batch 220/2240, Loss: 0.0055\nEpoch 8, Batch 230/2240, Loss: 0.0026\nEpoch 8, Batch 240/2240, Loss: 0.0512\nEpoch 8, Batch 250/2240, Loss: 0.2625\nEpoch 8, Batch 260/2240, Loss: 0.0305\nEpoch 8, Batch 270/2240, Loss: 0.1452\nEpoch 8, Batch 280/2240, Loss: 0.0072\nEpoch 8, Batch 290/2240, Loss: 0.0119\nEpoch 8, Batch 300/2240, Loss: 0.0216\nEpoch 8, Batch 310/2240, Loss: 0.0057\nEpoch 8, Batch 320/2240, Loss: 0.0269\nEpoch 8, Batch 330/2240, Loss: 0.0045\nEpoch 8, Batch 340/2240, Loss: 0.0436\nEpoch 8, Batch 350/2240, Loss: 0.5250\nEpoch 8, Batch 360/2240, Loss: 0.0040\nEpoch 8, Batch 370/2240, Loss: 0.0017\nEpoch 8, Batch 380/2240, Loss: 0.0040\nEpoch 8, Batch 390/2240, Loss: 0.0016\nEpoch 8, Batch 400/2240, Loss: 0.0008\nEpoch 8, Batch 410/2240, Loss: 0.0062\nEpoch 8, Batch 420/2240, Loss: 0.2741\nEpoch 8, Batch 430/2240, Loss: 0.0254\nEpoch 8, Batch 440/2240, Loss: 0.0952\nEpoch 8, Batch 450/2240, Loss: 0.0275\nEpoch 8, Batch 460/2240, Loss: 0.0059\nEpoch 8, Batch 470/2240, Loss: 0.0155\nEpoch 8, Batch 480/2240, Loss: 0.1165\nEpoch 8, Batch 490/2240, Loss: 0.0033\nEpoch 8, Batch 500/2240, Loss: 0.0079\nEpoch 8, Batch 510/2240, Loss: 0.0051\nEpoch 8, Batch 520/2240, Loss: 0.0224\nEpoch 8, Batch 530/2240, Loss: 0.0108\nEpoch 8, Batch 540/2240, Loss: 0.0170\nEpoch 8, Batch 550/2240, Loss: 0.0371\nEpoch 8, Batch 560/2240, Loss: 1.1792\nEpoch 8, Batch 570/2240, Loss: 0.0011\nEpoch 8, Batch 580/2240, Loss: 0.0085\nEpoch 8, Batch 590/2240, Loss: 0.0072\nEpoch 8, Batch 600/2240, Loss: 0.0129\nEpoch 8, Batch 610/2240, Loss: 0.2075\nEpoch 8, Batch 620/2240, Loss: 0.0185\nEpoch 8, Batch 630/2240, Loss: 0.0686\nEpoch 8, Batch 640/2240, Loss: 0.0008\nEpoch 8, Batch 650/2240, Loss: 0.0009\nEpoch 8, Batch 660/2240, Loss: 0.0777\nEpoch 8, Batch 670/2240, Loss: 0.0015\nEpoch 8, Batch 680/2240, Loss: 0.0227\nEpoch 8, Batch 690/2240, Loss: 0.0005\nEpoch 8, Batch 700/2240, Loss: 0.0292\nEpoch 8, Batch 710/2240, Loss: 0.0024\nEpoch 8, Batch 720/2240, Loss: 0.0093\nEpoch 8, Batch 730/2240, Loss: 0.0473\nEpoch 8, Batch 740/2240, Loss: 0.0103\nEpoch 8, Batch 750/2240, Loss: 0.0008\nEpoch 8, Batch 760/2240, Loss: 0.0015\nEpoch 8, Batch 770/2240, Loss: 0.0314\nEpoch 8, Batch 780/2240, Loss: 0.0618\nEpoch 8, Batch 790/2240, Loss: 0.0207\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Batch 800/2240, Loss: 0.0032\nEpoch 8, Batch 810/2240, Loss: 0.0005\nEpoch 8, Batch 820/2240, Loss: 0.0653\nEpoch 8, Batch 830/2240, Loss: 0.0083\nEpoch 8, Batch 840/2240, Loss: 0.0173\nEpoch 8, Batch 850/2240, Loss: 0.1988\nEpoch 8, Batch 860/2240, Loss: 0.0293\nEpoch 8, Batch 870/2240, Loss: 0.0475\nEpoch 8, Batch 880/2240, Loss: 0.0019\nEpoch 8, Batch 890/2240, Loss: 0.0066\nEpoch 8, Batch 900/2240, Loss: 0.0037\nEpoch 8, Batch 910/2240, Loss: 0.0296\nEpoch 8, Batch 920/2240, Loss: 0.0025\nEpoch 8, Batch 930/2240, Loss: 0.1727\nEpoch 8, Batch 940/2240, Loss: 0.0148\nEpoch 8, Batch 950/2240, Loss: 0.4436\nEpoch 8, Batch 960/2240, Loss: 0.5001\nEpoch 8, Batch 970/2240, Loss: 0.0318\nEpoch 8, Batch 980/2240, Loss: 0.0210\nEpoch 8, Batch 990/2240, Loss: 0.0056\nEpoch 8, Batch 1020/2240, Loss: 0.0023\nEpoch 8, Batch 1030/2240, Loss: 0.2467\nEpoch 8, Batch 1040/2240, Loss: 0.0007\nEpoch 8, Batch 1050/2240, Loss: 0.0157\nEpoch 8, Batch 1060/2240, Loss: 0.0009\nEpoch 8, Batch 1070/2240, Loss: 0.0258\nEpoch 8, Batch 1080/2240, Loss: 0.0082\nEpoch 8, Batch 1090/2240, Loss: 0.0028\nEpoch 8, Batch 1100/2240, Loss: 0.0210\nEpoch 8, Batch 1110/2240, Loss: 0.0713\nEpoch 8, Batch 1120/2240, Loss: 0.0124\nEpoch 8, Batch 1130/2240, Loss: 0.0073\nEpoch 8, Batch 1140/2240, Loss: 0.0300\nEpoch 8, Batch 1150/2240, Loss: 0.0020\nEpoch 8, Batch 1160/2240, Loss: 0.1710\nEpoch 8, Batch 1170/2240, Loss: 0.0008\nEpoch 8, Batch 1180/2240, Loss: 0.0023\nEpoch 8, Batch 1190/2240, Loss: 0.0033\nEpoch 8, Batch 1200/2240, Loss: 0.0528\nEpoch 8, Batch 1210/2240, Loss: 0.1025\nEpoch 8, Batch 1220/2240, Loss: 0.2464\nEpoch 8, Batch 1230/2240, Loss: 0.0030\nEpoch 8, Batch 1240/2240, Loss: 0.1642\nEpoch 8, Batch 1250/2240, Loss: 0.0315\nEpoch 8, Batch 1260/2240, Loss: 0.4821\nEpoch 8, Batch 1270/2240, Loss: 0.0067\nEpoch 8, Batch 1280/2240, Loss: 0.0276\nEpoch 8, Batch 1290/2240, Loss: 0.0764\nEpoch 8, Batch 1300/2240, Loss: 0.0392\nEpoch 8, Batch 1310/2240, Loss: 0.0013\nEpoch 8, Batch 1320/2240, Loss: 0.0087\nEpoch 8, Batch 1330/2240, Loss: 0.0435\nEpoch 8, Batch 1340/2240, Loss: 0.0015\nEpoch 8, Batch 1350/2240, Loss: 0.0027\nEpoch 8, Batch 1360/2240, Loss: 0.0022\nEpoch 8, Batch 1370/2240, Loss: 0.0324\nEpoch 8, Batch 1380/2240, Loss: 0.0047\nEpoch 8, Batch 1390/2240, Loss: 0.1044\nEpoch 8, Batch 1400/2240, Loss: 0.6438\nEpoch 8, Batch 1410/2240, Loss: 0.0004\nEpoch 8, Batch 1420/2240, Loss: 0.0018\nEpoch 8, Batch 1430/2240, Loss: 0.0384\nEpoch 8, Batch 1440/2240, Loss: 0.0449\nEpoch 8, Batch 1450/2240, Loss: 0.0088\nEpoch 8, Batch 1460/2240, Loss: 0.0020\nEpoch 8, Batch 1470/2240, Loss: 0.0358\nEpoch 8, Batch 1480/2240, Loss: 0.0242\nEpoch 8, Batch 1490/2240, Loss: 0.0338\nEpoch 8, Batch 1500/2240, Loss: 0.0007\nEpoch 8, Batch 1510/2240, Loss: 0.0261\nEpoch 8, Batch 1520/2240, Loss: 0.0022\nEpoch 8, Batch 1530/2240, Loss: 0.0216\nEpoch 8, Batch 1540/2240, Loss: 0.0116\nEpoch 8, Batch 1550/2240, Loss: 0.0026\nEpoch 8, Batch 1560/2240, Loss: 0.0013\nEpoch 8, Batch 1570/2240, Loss: 0.0123\nEpoch 8, Batch 1580/2240, Loss: 0.0011\nEpoch 8, Batch 1590/2240, Loss: 0.0029\nEpoch 8, Batch 1600/2240, Loss: 0.0070\nEpoch 8, Batch 1610/2240, Loss: 0.0534\nEpoch 8, Batch 1620/2240, Loss: 0.0589\nEpoch 8, Batch 1630/2240, Loss: 0.0100\nEpoch 8, Batch 1640/2240, Loss: 0.0182\nEpoch 8, Batch 1650/2240, Loss: 0.0068\nEpoch 8, Batch 1660/2240, Loss: 0.0028\nEpoch 8, Batch 1670/2240, Loss: 0.0164\nEpoch 8, Batch 1680/2240, Loss: 0.0280\nEpoch 8, Batch 1690/2240, Loss: 0.0064\nEpoch 8, Batch 1700/2240, Loss: 0.0007\nEpoch 8, Batch 1710/2240, Loss: 0.0009\nEpoch 8, Batch 1720/2240, Loss: 0.0030\nEpoch 8, Batch 1730/2240, Loss: 0.0029\nEpoch 8, Batch 1740/2240, Loss: 0.0032\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n  warnings.warn(str(msg))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Batch 1750/2240, Loss: 0.6541\nEpoch 8, Batch 1760/2240, Loss: 0.0246\nEpoch 8, Batch 1770/2240, Loss: 0.0011\nEpoch 8, Batch 1780/2240, Loss: 0.3261\nEpoch 8, Batch 1790/2240, Loss: 0.0021\nEpoch 8, Batch 1800/2240, Loss: 0.0852\nEpoch 8, Batch 1810/2240, Loss: 0.0003\nEpoch 8, Batch 1820/2240, Loss: 0.0689\nEpoch 8, Batch 1830/2240, Loss: 0.1873\nEpoch 8, Batch 1840/2240, Loss: 0.1440\nEpoch 8, Batch 1850/2240, Loss: 0.0044\nEpoch 8, Batch 1860/2240, Loss: 0.0017\nEpoch 8, Batch 1870/2240, Loss: 0.0166\nEpoch 8, Batch 1880/2240, Loss: 0.6302\nEpoch 8, Batch 1890/2240, Loss: 0.0007\nEpoch 8, Batch 1900/2240, Loss: 0.0137\nEpoch 8, Batch 1910/2240, Loss: 0.0116\nEpoch 8, Batch 1920/2240, Loss: 0.1893\nEpoch 8, Batch 1930/2240, Loss: 0.0045\nEpoch 8, Batch 1940/2240, Loss: 0.3176\nEpoch 8, Batch 1950/2240, Loss: 0.0300\nEpoch 8, Batch 1960/2240, Loss: 0.0424\nEpoch 8, Batch 1970/2240, Loss: 0.0197\nEpoch 8, Batch 1980/2240, Loss: 0.0015\nEpoch 8, Batch 1990/2240, Loss: 0.0030\nEpoch 8, Batch 2000/2240, Loss: 0.0058\nEpoch 8, Batch 2010/2240, Loss: 0.0148\nEpoch 8, Batch 2020/2240, Loss: 0.0285\nEpoch 8, Batch 2030/2240, Loss: 0.0288\nEpoch 8, Batch 2040/2240, Loss: 0.0093\nEpoch 8, Batch 2050/2240, Loss: 0.0176\nEpoch 8, Batch 2060/2240, Loss: 0.1788\nEpoch 8, Batch 2070/2240, Loss: 0.0268\nEpoch 8, Batch 2080/2240, Loss: 0.0750\nEpoch 8, Batch 2090/2240, Loss: 0.0050\nEpoch 8, Batch 2100/2240, Loss: 0.0039\nEpoch 8, Batch 2110/2240, Loss: 0.0051\nEpoch 8, Batch 2120/2240, Loss: 0.2611\nEpoch 8, Batch 2130/2240, Loss: 0.0033\nEpoch 8, Batch 2140/2240, Loss: 0.0021\nEpoch 8, Batch 2150/2240, Loss: 0.0016\nEpoch 8, Batch 2160/2240, Loss: 0.1602\nEpoch 8, Batch 2170/2240, Loss: 0.1499\nEpoch 8, Batch 2180/2240, Loss: 0.0621\nEpoch 8, Batch 2190/2240, Loss: 0.0069\nEpoch 8, Batch 2200/2240, Loss: 0.0054\nEpoch 8, Batch 2210/2240, Loss: 0.3121\nEpoch 8, Batch 2220/2240, Loss: 0.0033\nEpoch 8, Batch 2230/2240, Loss: 0.0064\nEpoch 8, Batch 2240/2240, Loss: 0.0159\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Train Loss: 0.0525, Train Acc: 0.9821, Valid Loss: 0.9034, Valid Acc: 0.7935\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Batch 10/2240, Loss: 0.0538\nEpoch 9, Batch 20/2240, Loss: 0.0004\nEpoch 9, Batch 30/2240, Loss: 0.3922\nEpoch 9, Batch 40/2240, Loss: 0.0083\nEpoch 9, Batch 50/2240, Loss: 0.2319\nEpoch 9, Batch 60/2240, Loss: 0.0656\nEpoch 9, Batch 70/2240, Loss: 0.0193\nEpoch 9, Batch 80/2240, Loss: 0.0223\nEpoch 9, Batch 90/2240, Loss: 0.0390\nEpoch 9, Batch 100/2240, Loss: 0.0163\nEpoch 9, Batch 110/2240, Loss: 0.0006\nEpoch 9, Batch 120/2240, Loss: 0.0062\nEpoch 9, Batch 130/2240, Loss: 0.0550\nEpoch 9, Batch 140/2240, Loss: 0.0006\nEpoch 9, Batch 150/2240, Loss: 0.1829\nEpoch 9, Batch 160/2240, Loss: 0.0014\nEpoch 9, Batch 170/2240, Loss: 0.1671\nEpoch 9, Batch 180/2240, Loss: 0.0039\nEpoch 9, Batch 190/2240, Loss: 0.0017\nEpoch 9, Batch 200/2240, Loss: 0.0004\nEpoch 9, Batch 210/2240, Loss: 0.1727\nEpoch 9, Batch 220/2240, Loss: 0.0023\nEpoch 9, Batch 230/2240, Loss: 0.0002\nEpoch 9, Batch 240/2240, Loss: 0.0017\nEpoch 9, Batch 250/2240, Loss: 0.0117\nEpoch 9, Batch 260/2240, Loss: 0.0549\nEpoch 9, Batch 270/2240, Loss: 0.0056\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Batch 280/2240, Loss: 0.1837\nEpoch 9, Batch 290/2240, Loss: 0.0933\nEpoch 9, Batch 300/2240, Loss: 0.0024\nEpoch 9, Batch 310/2240, Loss: 0.0001\nEpoch 9, Batch 320/2240, Loss: 0.0080\nEpoch 9, Batch 330/2240, Loss: 0.1723\nEpoch 9, Batch 340/2240, Loss: 0.0125\nEpoch 9, Batch 350/2240, Loss: 0.0011\nEpoch 9, Batch 360/2240, Loss: 0.0005\nEpoch 9, Batch 370/2240, Loss: 0.0015\nEpoch 9, Batch 380/2240, Loss: 0.0102\nEpoch 9, Batch 390/2240, Loss: 0.0050\nEpoch 9, Batch 400/2240, Loss: 0.0196\nEpoch 9, Batch 410/2240, Loss: 0.0502\nEpoch 9, Batch 420/2240, Loss: 0.0173\nEpoch 9, Batch 430/2240, Loss: 0.0358\nEpoch 9, Batch 440/2240, Loss: 0.0005\nEpoch 9, Batch 450/2240, Loss: 0.0094\nEpoch 9, Batch 460/2240, Loss: 0.0011\nEpoch 9, Batch 470/2240, Loss: 0.0098\nEpoch 9, Batch 480/2240, Loss: 0.0050\nEpoch 9, Batch 490/2240, Loss: 0.0133\nEpoch 9, Batch 500/2240, Loss: 0.0025\nEpoch 9, Batch 510/2240, Loss: 0.0065\nEpoch 9, Batch 520/2240, Loss: 0.0066\nEpoch 9, Batch 530/2240, Loss: 0.0247\nEpoch 9, Batch 540/2240, Loss: 0.4193\nEpoch 9, Batch 550/2240, Loss: 0.0085\nEpoch 9, Batch 560/2240, Loss: 0.0010\nEpoch 9, Batch 570/2240, Loss: 0.0075\nEpoch 9, Batch 580/2240, Loss: 0.0008\nEpoch 9, Batch 590/2240, Loss: 0.0428\nEpoch 9, Batch 600/2240, Loss: 0.0145\nEpoch 9, Batch 610/2240, Loss: 0.0795\nEpoch 9, Batch 620/2240, Loss: 0.0005\nEpoch 9, Batch 630/2240, Loss: 0.0016\nEpoch 9, Batch 640/2240, Loss: 0.0088\nEpoch 9, Batch 650/2240, Loss: 0.3189\nEpoch 9, Batch 660/2240, Loss: 0.0123\nEpoch 9, Batch 670/2240, Loss: 0.0090\nEpoch 9, Batch 680/2240, Loss: 0.0028\nEpoch 9, Batch 690/2240, Loss: 0.0023\nEpoch 9, Batch 700/2240, Loss: 0.0006\nEpoch 9, Batch 710/2240, Loss: 0.0353\nEpoch 9, Batch 720/2240, Loss: 0.0324\nEpoch 9, Batch 730/2240, Loss: 0.0004\nEpoch 9, Batch 740/2240, Loss: 0.0004\nEpoch 9, Batch 750/2240, Loss: 0.6126\nEpoch 9, Batch 760/2240, Loss: 0.0082\nEpoch 9, Batch 770/2240, Loss: 0.0011\nEpoch 9, Batch 780/2240, Loss: 0.0095\nEpoch 9, Batch 790/2240, Loss: 0.0010\nEpoch 9, Batch 800/2240, Loss: 0.0014\nEpoch 9, Batch 810/2240, Loss: 0.0119\nEpoch 9, Batch 820/2240, Loss: 0.0008\nEpoch 9, Batch 830/2240, Loss: 0.0101\nEpoch 9, Batch 840/2240, Loss: 0.0186\nEpoch 9, Batch 850/2240, Loss: 0.0060\nEpoch 9, Batch 860/2240, Loss: 0.0075\nEpoch 9, Batch 870/2240, Loss: 0.0058\nEpoch 9, Batch 880/2240, Loss: 0.0032\nEpoch 9, Batch 890/2240, Loss: 0.0064\nEpoch 9, Batch 900/2240, Loss: 0.0005\nEpoch 9, Batch 910/2240, Loss: 0.0012\nEpoch 9, Batch 920/2240, Loss: 0.0208\nEpoch 9, Batch 930/2240, Loss: 0.0019\nEpoch 9, Batch 940/2240, Loss: 0.0281\nEpoch 9, Batch 950/2240, Loss: 0.0345\nEpoch 9, Batch 960/2240, Loss: 0.0235\nEpoch 9, Batch 970/2240, Loss: 0.0045\nEpoch 9, Batch 980/2240, Loss: 0.0156\nEpoch 9, Batch 990/2240, Loss: 0.0103\nEpoch 9, Batch 1000/2240, Loss: 0.0086\nEpoch 9, Batch 1010/2240, Loss: 0.0001\nEpoch 9, Batch 1020/2240, Loss: 0.0036\nEpoch 9, Batch 1030/2240, Loss: 0.0172\nEpoch 9, Batch 1040/2240, Loss: 0.0646\nEpoch 9, Batch 1050/2240, Loss: 0.0064\nEpoch 9, Batch 1060/2240, Loss: 0.0033\nEpoch 9, Batch 1070/2240, Loss: 0.0140\nEpoch 9, Batch 1080/2240, Loss: 0.0205\nEpoch 9, Batch 1090/2240, Loss: 0.0059\nEpoch 9, Batch 1100/2240, Loss: 0.0048\nEpoch 9, Batch 1110/2240, Loss: 0.0017\nEpoch 9, Batch 1120/2240, Loss: 0.0040\nEpoch 9, Batch 1130/2240, Loss: 0.0018\nEpoch 9, Batch 1140/2240, Loss: 0.0174\nEpoch 9, Batch 1150/2240, Loss: 0.1506\nEpoch 9, Batch 1160/2240, Loss: 0.0038\nEpoch 9, Batch 1170/2240, Loss: 0.0095\nEpoch 9, Batch 1180/2240, Loss: 0.0479\nEpoch 9, Batch 1190/2240, Loss: 0.0018\nEpoch 9, Batch 1200/2240, Loss: 0.0319\nEpoch 9, Batch 1210/2240, Loss: 0.0337\nEpoch 9, Batch 1220/2240, Loss: 0.0087\nEpoch 9, Batch 1230/2240, Loss: 0.0039\nEpoch 9, Batch 1240/2240, Loss: 0.0445\nEpoch 9, Batch 1250/2240, Loss: 0.0047\nEpoch 9, Batch 1260/2240, Loss: 0.0170\nEpoch 9, Batch 1270/2240, Loss: 0.1002\nEpoch 9, Batch 1280/2240, Loss: 0.0425\nEpoch 9, Batch 1290/2240, Loss: 0.0003\nEpoch 9, Batch 1300/2240, Loss: 0.0005\nEpoch 9, Batch 1310/2240, Loss: 0.0177\nEpoch 9, Batch 1320/2240, Loss: 0.0710\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n  warnings.warn(str(msg))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Batch 1330/2240, Loss: 0.0022\nEpoch 9, Batch 1340/2240, Loss: 0.0072\nEpoch 9, Batch 1350/2240, Loss: 0.1178\nEpoch 9, Batch 1360/2240, Loss: 0.0011\nEpoch 9, Batch 1370/2240, Loss: 0.1464\nEpoch 9, Batch 1380/2240, Loss: 0.0008\nEpoch 9, Batch 1390/2240, Loss: 0.1452\nEpoch 9, Batch 1400/2240, Loss: 0.0145\nEpoch 9, Batch 1410/2240, Loss: 0.0994\nEpoch 9, Batch 1420/2240, Loss: 0.0072\nEpoch 9, Batch 1430/2240, Loss: 0.0066\nEpoch 9, Batch 1440/2240, Loss: 0.2380\nEpoch 9, Batch 1450/2240, Loss: 0.0297\nEpoch 9, Batch 1460/2240, Loss: 0.0023\nEpoch 9, Batch 1470/2240, Loss: 0.0121\nEpoch 9, Batch 1480/2240, Loss: 0.0520\nEpoch 9, Batch 1490/2240, Loss: 0.0028\nEpoch 9, Batch 1500/2240, Loss: 0.0062\nEpoch 9, Batch 1510/2240, Loss: 0.0009\nEpoch 9, Batch 1520/2240, Loss: 0.0530\nEpoch 9, Batch 1530/2240, Loss: 0.0048\nEpoch 9, Batch 1540/2240, Loss: 0.0064\nEpoch 9, Batch 1550/2240, Loss: 0.0044\nEpoch 9, Batch 1560/2240, Loss: 0.0329\nEpoch 9, Batch 1570/2240, Loss: 0.0360\nEpoch 9, Batch 1580/2240, Loss: 0.3847\nEpoch 9, Batch 1590/2240, Loss: 0.0287\nEpoch 9, Batch 1600/2240, Loss: 0.3515\nEpoch 9, Batch 1610/2240, Loss: 0.0028\nEpoch 9, Batch 1620/2240, Loss: 0.0023\nEpoch 9, Batch 1630/2240, Loss: 0.0029\nEpoch 9, Batch 1640/2240, Loss: 0.0002\nEpoch 9, Batch 1650/2240, Loss: 0.0289\nEpoch 9, Batch 1660/2240, Loss: 0.0066\nEpoch 9, Batch 1670/2240, Loss: 0.3588\nEpoch 9, Batch 1680/2240, Loss: 0.0005\nEpoch 9, Batch 1690/2240, Loss: 0.0483\nEpoch 9, Batch 1700/2240, Loss: 0.0046\nEpoch 9, Batch 1710/2240, Loss: 0.0035\nEpoch 9, Batch 1720/2240, Loss: 0.0137\nEpoch 9, Batch 1730/2240, Loss: 0.0020\nEpoch 9, Batch 1740/2240, Loss: 0.0006\nEpoch 9, Batch 1750/2240, Loss: 0.0042\nEpoch 9, Batch 1760/2240, Loss: 0.0210\nEpoch 9, Batch 1770/2240, Loss: 0.0004\nEpoch 9, Batch 1780/2240, Loss: 0.0774\nEpoch 9, Batch 1790/2240, Loss: 0.0026\nEpoch 9, Batch 1800/2240, Loss: 0.0002\nEpoch 9, Batch 1810/2240, Loss: 0.6926\nEpoch 9, Batch 1820/2240, Loss: 0.0026\nEpoch 9, Batch 1830/2240, Loss: 0.0025\nEpoch 9, Batch 1840/2240, Loss: 0.0075\nEpoch 9, Batch 1850/2240, Loss: 0.0022\nEpoch 9, Batch 1860/2240, Loss: 0.0043\nEpoch 9, Batch 1870/2240, Loss: 0.0471\nEpoch 9, Batch 1880/2240, Loss: 0.0018\nEpoch 9, Batch 1890/2240, Loss: 0.0264\nEpoch 9, Batch 1900/2240, Loss: 0.0033\nEpoch 9, Batch 1910/2240, Loss: 0.0004\nEpoch 9, Batch 1920/2240, Loss: 0.0024\nEpoch 9, Batch 1930/2240, Loss: 0.0215\nEpoch 9, Batch 1940/2240, Loss: 0.1328\nEpoch 9, Batch 1950/2240, Loss: 0.0082\nEpoch 9, Batch 1960/2240, Loss: 0.1120\nEpoch 9, Batch 1970/2240, Loss: 0.2057\nEpoch 9, Batch 1980/2240, Loss: 0.0095\nEpoch 9, Batch 1990/2240, Loss: 0.0040\nEpoch 9, Batch 2000/2240, Loss: 0.1908\nEpoch 9, Batch 2010/2240, Loss: 0.0005\nEpoch 9, Batch 2020/2240, Loss: 0.0204\nEpoch 9, Batch 2030/2240, Loss: 0.0077\nEpoch 9, Batch 2040/2240, Loss: 0.0158\nEpoch 9, Batch 2050/2240, Loss: 0.0637\nEpoch 9, Batch 2060/2240, Loss: 0.0054\nEpoch 9, Batch 2070/2240, Loss: 0.0105\nEpoch 9, Batch 2080/2240, Loss: 0.0008\nEpoch 9, Batch 2090/2240, Loss: 0.0136\nEpoch 9, Batch 2100/2240, Loss: 0.0026\nEpoch 9, Batch 2110/2240, Loss: 0.0036\nEpoch 9, Batch 2120/2240, Loss: 0.0046\nEpoch 9, Batch 2130/2240, Loss: 0.0063\nEpoch 9, Batch 2140/2240, Loss: 0.1166\nEpoch 9, Batch 2150/2240, Loss: 0.0060\nEpoch 9, Batch 2160/2240, Loss: 0.0008\nEpoch 9, Batch 2170/2240, Loss: 0.0080\nEpoch 9, Batch 2180/2240, Loss: 0.0044\nEpoch 9, Batch 2190/2240, Loss: 0.0171\nEpoch 9, Batch 2200/2240, Loss: 0.0273\nEpoch 9, Batch 2210/2240, Loss: 0.5255\nEpoch 9, Batch 2220/2240, Loss: 0.0909\nEpoch 9, Batch 2230/2240, Loss: 0.2619\nEpoch 9, Batch 2240/2240, Loss: 0.0027\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Train Loss: 0.0446, Train Acc: 0.9850, Valid Loss: 0.8906, Valid Acc: 0.7932\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Batch 10/2240, Loss: 0.0059\nEpoch 10, Batch 20/2240, Loss: 0.0017\nEpoch 10, Batch 30/2240, Loss: 0.0027\nEpoch 10, Batch 40/2240, Loss: 0.0105\nEpoch 10, Batch 50/2240, Loss: 0.0034\nEpoch 10, Batch 60/2240, Loss: 0.0005\nEpoch 10, Batch 70/2240, Loss: 0.0004\nEpoch 10, Batch 80/2240, Loss: 0.0031\nEpoch 10, Batch 90/2240, Loss: 0.0133\nEpoch 10, Batch 100/2240, Loss: 0.0018\nEpoch 10, Batch 110/2240, Loss: 0.0070\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n  warnings.warn(str(msg))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Batch 120/2240, Loss: 0.0005\nEpoch 10, Batch 130/2240, Loss: 0.0690\nEpoch 10, Batch 140/2240, Loss: 0.0060\nEpoch 10, Batch 150/2240, Loss: 0.0017\nEpoch 10, Batch 160/2240, Loss: 0.0036\nEpoch 10, Batch 170/2240, Loss: 0.0448\nEpoch 10, Batch 180/2240, Loss: 0.0153\nEpoch 10, Batch 190/2240, Loss: 0.0214\nEpoch 10, Batch 200/2240, Loss: 0.0064\nEpoch 10, Batch 210/2240, Loss: 0.0232\nEpoch 10, Batch 220/2240, Loss: 0.0088\nEpoch 10, Batch 230/2240, Loss: 0.0015\nEpoch 10, Batch 240/2240, Loss: 0.0088\nEpoch 10, Batch 250/2240, Loss: 0.0067\nEpoch 10, Batch 260/2240, Loss: 0.0081\nEpoch 10, Batch 270/2240, Loss: 0.0018\nEpoch 10, Batch 280/2240, Loss: 0.2103\nEpoch 10, Batch 290/2240, Loss: 0.0057\nEpoch 10, Batch 300/2240, Loss: 0.0141\nEpoch 10, Batch 310/2240, Loss: 0.0021\nEpoch 10, Batch 320/2240, Loss: 0.0687\nEpoch 10, Batch 330/2240, Loss: 0.0009\nEpoch 10, Batch 340/2240, Loss: 0.0067\nEpoch 10, Batch 350/2240, Loss: 0.0018\nEpoch 10, Batch 360/2240, Loss: 0.0134\nEpoch 10, Batch 370/2240, Loss: 0.0006\nEpoch 10, Batch 380/2240, Loss: 0.0932\nEpoch 10, Batch 390/2240, Loss: 0.0003\nEpoch 10, Batch 400/2240, Loss: 0.0026\nEpoch 10, Batch 410/2240, Loss: 0.0024\nEpoch 10, Batch 420/2240, Loss: 0.0040\nEpoch 10, Batch 430/2240, Loss: 0.0002\nEpoch 10, Batch 440/2240, Loss: 0.0276\nEpoch 10, Batch 450/2240, Loss: 0.0007\nEpoch 10, Batch 460/2240, Loss: 0.0005\nEpoch 10, Batch 470/2240, Loss: 0.0114\nEpoch 10, Batch 480/2240, Loss: 0.0020\nEpoch 10, Batch 490/2240, Loss: 0.0519\nEpoch 10, Batch 500/2240, Loss: 0.0021\nEpoch 10, Batch 510/2240, Loss: 0.0030\nEpoch 10, Batch 520/2240, Loss: 0.0299\nEpoch 10, Batch 530/2240, Loss: 0.0764\nEpoch 10, Batch 540/2240, Loss: 0.0004\nEpoch 10, Batch 550/2240, Loss: 0.0018\nEpoch 10, Batch 560/2240, Loss: 0.0044\nEpoch 10, Batch 570/2240, Loss: 0.1090\nEpoch 10, Batch 580/2240, Loss: 0.0439\nEpoch 10, Batch 590/2240, Loss: 0.0037\nEpoch 10, Batch 600/2240, Loss: 0.0025\nEpoch 10, Batch 610/2240, Loss: 0.0157\nEpoch 10, Batch 620/2240, Loss: 0.0016\nEpoch 10, Batch 630/2240, Loss: 0.0763\nEpoch 10, Batch 640/2240, Loss: 0.0023\nEpoch 10, Batch 650/2240, Loss: 0.0001\nEpoch 10, Batch 660/2240, Loss: 0.0263\nEpoch 10, Batch 670/2240, Loss: 0.0047\nEpoch 10, Batch 680/2240, Loss: 0.3083\nEpoch 10, Batch 690/2240, Loss: 0.0050\nEpoch 10, Batch 700/2240, Loss: 0.0043\nEpoch 10, Batch 710/2240, Loss: 0.0004\nEpoch 10, Batch 720/2240, Loss: 0.0007\nEpoch 10, Batch 730/2240, Loss: 0.0037\nEpoch 10, Batch 740/2240, Loss: 0.0375\nEpoch 10, Batch 750/2240, Loss: 0.0025\nEpoch 10, Batch 760/2240, Loss: 0.0015\nEpoch 10, Batch 770/2240, Loss: 0.0016\nEpoch 10, Batch 780/2240, Loss: 0.0017\nEpoch 10, Batch 790/2240, Loss: 0.0003\nEpoch 10, Batch 800/2240, Loss: 0.0002\nEpoch 10, Batch 810/2240, Loss: 0.0045\nEpoch 10, Batch 820/2240, Loss: 0.0002\nEpoch 10, Batch 830/2240, Loss: 0.3352\nEpoch 10, Batch 840/2240, Loss: 0.0046\nEpoch 10, Batch 850/2240, Loss: 0.0102\nEpoch 10, Batch 860/2240, Loss: 0.0254\nEpoch 10, Batch 870/2240, Loss: 0.0008\nEpoch 10, Batch 880/2240, Loss: 0.1165\nEpoch 10, Batch 890/2240, Loss: 0.0080\nEpoch 10, Batch 900/2240, Loss: 0.5567\nEpoch 10, Batch 910/2240, Loss: 0.0037\nEpoch 10, Batch 920/2240, Loss: 0.0025\nEpoch 10, Batch 930/2240, Loss: 0.0010\nEpoch 10, Batch 940/2240, Loss: 0.0037\nEpoch 10, Batch 950/2240, Loss: 0.0003\nEpoch 10, Batch 960/2240, Loss: 0.0038\nEpoch 10, Batch 970/2240, Loss: 0.0054\nEpoch 10, Batch 980/2240, Loss: 0.0001\nEpoch 10, Batch 990/2240, Loss: 0.0023\nEpoch 10, Batch 1000/2240, Loss: 0.2306\nEpoch 10, Batch 1010/2240, Loss: 0.0227\nEpoch 10, Batch 1020/2240, Loss: 0.4498\nEpoch 10, Batch 1030/2240, Loss: 0.0014\nEpoch 10, Batch 1040/2240, Loss: 0.0003\nEpoch 10, Batch 1050/2240, Loss: 0.0006\nEpoch 10, Batch 1060/2240, Loss: 0.0080\nEpoch 10, Batch 1070/2240, Loss: 0.0184\nEpoch 10, Batch 1080/2240, Loss: 0.0004\nEpoch 10, Batch 1090/2240, Loss: 0.0074\nEpoch 10, Batch 1100/2240, Loss: 0.0023\nEpoch 10, Batch 1110/2240, Loss: 0.0015\nEpoch 10, Batch 1120/2240, Loss: 0.0042\nEpoch 10, Batch 1130/2240, Loss: 0.0035\nEpoch 10, Batch 1140/2240, Loss: 0.5097\nEpoch 10, Batch 1150/2240, Loss: 0.0171\nEpoch 10, Batch 1160/2240, Loss: 0.0020\nEpoch 10, Batch 1170/2240, Loss: 0.0060\nEpoch 10, Batch 1180/2240, Loss: 0.0028\nEpoch 10, Batch 1190/2240, Loss: 0.0029\nEpoch 10, Batch 1200/2240, Loss: 0.0011\nEpoch 10, Batch 1210/2240, Loss: 0.0182\nEpoch 10, Batch 1220/2240, Loss: 0.0030\nEpoch 10, Batch 1230/2240, Loss: 0.0001\nEpoch 10, Batch 1240/2240, Loss: 0.0021\nEpoch 10, Batch 1250/2240, Loss: 0.1111\nEpoch 10, Batch 1260/2240, Loss: 0.0001\nEpoch 10, Batch 1270/2240, Loss: 0.0014\nEpoch 10, Batch 1280/2240, Loss: 0.1320\nEpoch 10, Batch 1290/2240, Loss: 0.0002\nEpoch 10, Batch 1300/2240, Loss: 0.0021\nEpoch 10, Batch 1310/2240, Loss: 0.0065\nEpoch 10, Batch 1320/2240, Loss: 0.0019\nEpoch 10, Batch 1330/2240, Loss: 0.0012\nEpoch 10, Batch 1340/2240, Loss: 0.0018\nEpoch 10, Batch 1350/2240, Loss: 0.0065\nEpoch 10, Batch 1360/2240, Loss: 0.7532\nEpoch 10, Batch 1370/2240, Loss: 0.0005\nEpoch 10, Batch 1380/2240, Loss: 0.0022\nEpoch 10, Batch 1390/2240, Loss: 0.0025\nEpoch 10, Batch 1400/2240, Loss: 0.0033\nEpoch 10, Batch 1410/2240, Loss: 0.0002\nEpoch 10, Batch 1420/2240, Loss: 0.0002\nEpoch 10, Batch 1430/2240, Loss: 0.0014\nEpoch 10, Batch 1440/2240, Loss: 0.0009\nEpoch 10, Batch 1450/2240, Loss: 0.0153\nEpoch 10, Batch 1460/2240, Loss: 0.0408\nEpoch 10, Batch 1470/2240, Loss: 0.3113\nEpoch 10, Batch 1480/2240, Loss: 0.0028\nEpoch 10, Batch 1490/2240, Loss: 0.0088\nEpoch 10, Batch 1500/2240, Loss: 0.0207\nEpoch 10, Batch 1510/2240, Loss: 0.0002\nEpoch 10, Batch 1520/2240, Loss: 0.0755\nEpoch 10, Batch 1530/2240, Loss: 0.0026\nEpoch 10, Batch 1540/2240, Loss: 0.0066\nEpoch 10, Batch 1550/2240, Loss: 0.1497\nEpoch 10, Batch 1560/2240, Loss: 0.0573\nEpoch 10, Batch 1570/2240, Loss: 0.0011\nEpoch 10, Batch 1580/2240, Loss: 0.0584\nEpoch 10, Batch 1590/2240, Loss: 0.0115\nEpoch 10, Batch 1600/2240, Loss: 0.2862\nEpoch 10, Batch 1610/2240, Loss: 0.0111\nEpoch 10, Batch 1620/2240, Loss: 0.0167\nEpoch 10, Batch 1630/2240, Loss: 0.0092\nEpoch 10, Batch 1640/2240, Loss: 0.0073\nEpoch 10, Batch 1650/2240, Loss: 0.5076\nEpoch 10, Batch 1660/2240, Loss: 0.1282\nEpoch 10, Batch 1670/2240, Loss: 0.1723\nEpoch 10, Batch 1680/2240, Loss: 0.0009\nEpoch 10, Batch 1690/2240, Loss: 0.0008\nEpoch 10, Batch 1700/2240, Loss: 0.0013\nEpoch 10, Batch 1710/2240, Loss: 0.0178\nEpoch 10, Batch 1720/2240, Loss: 0.0413\nEpoch 10, Batch 1730/2240, Loss: 0.0107\nEpoch 10, Batch 1740/2240, Loss: 0.0001\nEpoch 10, Batch 1750/2240, Loss: 0.0047\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Batch 1760/2240, Loss: 0.0035\nEpoch 10, Batch 1770/2240, Loss: 0.0003\nEpoch 10, Batch 1780/2240, Loss: 0.0021\nEpoch 10, Batch 1790/2240, Loss: 0.0137\nEpoch 10, Batch 1800/2240, Loss: 0.0004\nEpoch 10, Batch 1810/2240, Loss: 0.0014\nEpoch 10, Batch 1820/2240, Loss: 0.0298\nEpoch 10, Batch 1830/2240, Loss: 0.0019\nEpoch 10, Batch 1840/2240, Loss: 0.0014\nEpoch 10, Batch 1850/2240, Loss: 0.0263\nEpoch 10, Batch 1860/2240, Loss: 0.0023\nEpoch 10, Batch 1870/2240, Loss: 0.0028\nEpoch 10, Batch 1880/2240, Loss: 0.0073\nEpoch 10, Batch 1890/2240, Loss: 0.4313\nEpoch 10, Batch 1900/2240, Loss: 0.0180\nEpoch 10, Batch 1910/2240, Loss: 0.0132\nEpoch 10, Batch 1920/2240, Loss: 0.0033\nEpoch 10, Batch 1930/2240, Loss: 0.0046\nEpoch 10, Batch 1940/2240, Loss: 0.0001\nEpoch 10, Batch 1950/2240, Loss: 0.0002\nEpoch 10, Batch 1960/2240, Loss: 0.0000\nEpoch 10, Batch 1970/2240, Loss: 0.0240\nEpoch 10, Batch 1980/2240, Loss: 0.1665\nEpoch 10, Batch 1990/2240, Loss: 0.0038\nEpoch 10, Batch 2000/2240, Loss: 0.0429\nEpoch 10, Batch 2010/2240, Loss: 0.0068\nEpoch 10, Batch 2020/2240, Loss: 0.0861\nEpoch 10, Batch 2030/2240, Loss: 0.0137\nEpoch 10, Batch 2040/2240, Loss: 0.2219\nEpoch 10, Batch 2050/2240, Loss: 0.0015\nEpoch 10, Batch 2060/2240, Loss: 0.0000\nEpoch 10, Batch 2070/2240, Loss: 0.0260\nEpoch 10, Batch 2080/2240, Loss: 0.1448\nEpoch 10, Batch 2090/2240, Loss: 0.0004\nEpoch 10, Batch 2100/2240, Loss: 0.0124\nEpoch 10, Batch 2110/2240, Loss: 0.5335\nEpoch 10, Batch 2120/2240, Loss: 0.0043\nEpoch 10, Batch 2130/2240, Loss: 0.0004\nEpoch 10, Batch 2140/2240, Loss: 0.0037\nEpoch 10, Batch 2150/2240, Loss: 0.0201\nEpoch 10, Batch 2160/2240, Loss: 0.0021\nEpoch 10, Batch 2170/2240, Loss: 0.0203\nEpoch 10, Batch 2180/2240, Loss: 0.0025\nEpoch 10, Batch 2190/2240, Loss: 0.0241\nEpoch 10, Batch 2200/2240, Loss: 0.0200\nEpoch 10, Batch 2210/2240, Loss: 0.0356\nEpoch 10, Batch 2220/2240, Loss: 0.0046\nEpoch 10, Batch 2230/2240, Loss: 0.0016\nEpoch 10, Batch 2240/2240, Loss: 0.0109\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Train Loss: 0.0319, Train Acc: 0.9890, Valid Loss: 1.0329, Valid Acc: 0.7802\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2193488528.py:468: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_multimodal_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluating model on test set...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.8120\n\nConfusion Matrix:\n[[ 874  358]\n [ 364 2244]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n Non-Harmful       0.71      0.71      0.71      1232\n     Harmful       0.86      0.86      0.86      2608\n\n    accuracy                           0.81      3840\n   macro avg       0.78      0.78      0.78      3840\nweighted avg       0.81      0.81      0.81      3840\n\nModel training and evaluation complete!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}